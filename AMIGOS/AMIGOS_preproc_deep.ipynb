{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import scipy.io\n",
    "import scipy.signal as sgl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import  Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Data Loading Function ---\n",
    "def load_patient_preprocessed_data(patient_number):\n",
    "    base_dir = r\"C:\\Users\\ferri\\Downloads\\PoliTO\\Tesi\\DSs\\Emotion-Stress\\AMIGOS\"\n",
    "    file_path = os.path.join(\n",
    "        base_dir, \"Data preprocessed\",\n",
    "        f\"Data_Preprocessed_P{patient_number:02d}\",\n",
    "        f\"Data_Preprocessed_P{patient_number:02d}.mat\"\n",
    "    )\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    return data\n",
    "\n",
    "# --- Preprocessing functions for pipeline ---\n",
    "def process_trial_signal(signal, target_length=None, fs=512):\n",
    "    \"\"\"\n",
    "    Convert a trial's raw signal into a 2D array [channels, time].\n",
    "    If target_length is None, we keep the full length (then you'll pad later).\n",
    "    Preprocessing steps (downsampling, filtering, baseline removal) are applied.\n",
    "    \"\"\"\n",
    "    # Convert to float32.\n",
    "    signal = np.array(signal, dtype=np.float32)\n",
    "    \n",
    "    # Check if signal is empty.\n",
    "    if signal.size == 0:\n",
    "        return np.empty((0,0), dtype=np.float32)\n",
    "    \n",
    "    # If the signal is 1D, reshape to (1, length).\n",
    "    if signal.ndim == 1:\n",
    "        signal = signal[None, :]\n",
    "    \n",
    "    # Downsampling parameters.\n",
    "    N = 4\n",
    "    lowcut, highcut = 1.0, 45.0\n",
    "    desired_fs = 128\n",
    "    down_factor = fs // desired_fs\n",
    "    \n",
    "    # 1) Downsample each channel.\n",
    "    downsampled = []\n",
    "    for ch_data in signal:\n",
    "        # If a channel is empty, skip it.\n",
    "        if ch_data.size == 0:\n",
    "            continue\n",
    "        ch_data_down = ch_data[::down_factor]\n",
    "        downsampled.append(ch_data_down)\n",
    "        \n",
    "    # If no channels had data, return an empty array.\n",
    "    if not downsampled:\n",
    "        return np.empty((0,0), dtype=np.float32)\n",
    "        \n",
    "    # Stack downsampled channels.\n",
    "    signal = np.vstack([ch[None, :] for ch in downsampled])\n",
    "    \n",
    "    # 2) Bandpass filter design.\n",
    "    nyquist = 0.5 * desired_fs\n",
    "    b, a = sgl.butter(N=4, Wn=[lowcut/nyquist, highcut/nyquist], btype='band')\n",
    "    \n",
    "    # Calculate the minimum length required by filtfilt.\n",
    "    min_len = 3 * (max(len(a), len(b)) - 1)\n",
    "    \n",
    "    # Filter each channel; if too short, skip filtering.\n",
    "    filtered = []\n",
    "    for ch_data in signal:\n",
    "        if len(ch_data) < min_len:\n",
    "            ch_data_filt = ch_data  # Fallback: leave unfiltered.\n",
    "        else:\n",
    "            ch_data_filt = sgl.filtfilt(b, a, ch_data)\n",
    "        filtered.append(ch_data_filt)\n",
    "    signal = np.vstack([ch[None, :] for ch in filtered])\n",
    "    \n",
    "    # 3) Baseline removal (subtract mean from each channel).\n",
    "    baseline_removed = []\n",
    "    for ch_data in signal:\n",
    "        ch_data_bs = ch_data - np.mean(ch_data)\n",
    "        baseline_removed.append(ch_data_bs)\n",
    "    signal = np.vstack([ch[None, :] for ch in baseline_removed])\n",
    "    \n",
    "    # 4) Padding/Truncation if target_length is provided.\n",
    "    if target_length is not None:\n",
    "        processed = []\n",
    "        for ch_data in signal:\n",
    "            ch_len = len(ch_data)\n",
    "            if ch_len == 0:\n",
    "                proc = np.zeros(target_length, dtype=np.float32)\n",
    "            elif ch_len < target_length:\n",
    "                pad_width = target_length - ch_len\n",
    "                proc = np.pad(ch_data, (0, pad_width), mode='edge')\n",
    "            else:\n",
    "                proc = ch_data[:target_length]\n",
    "            processed.append(proc.astype(np.float32))\n",
    "        signal = np.vstack([p[None, :] for p in processed])\n",
    "    \n",
    "    return signal.astype(np.float32)\n",
    "\n",
    "def split_into_modalities(signal):\n",
    "    # If the signal is 1D, assume it represents a single modality (e.g., ECG).\n",
    "    if signal.ndim == 1:\n",
    "        return {\"ecg\": signal}\n",
    "    else:\n",
    "        # If multi-channel, split into ECG, GSR, and EEG as desired.\n",
    "        ecg = signal[0, :]\n",
    "        gsr = signal[1, :]\n",
    "        eeg = signal[2, :] \n",
    "        return {\"ecg\": ecg, \"gsr\": gsr, \"eeg\": eeg}\n",
    "\n",
    "def discretize_label(label):\n",
    "    \"\"\"\n",
    "    Convert a label [valence, arousal] into a descriptive class.\n",
    "    If the flattened label has 2 elements, use them directly.\n",
    "    If it has 3 or more, use the second and third elements.\n",
    "    \"\"\"\n",
    "    flat_label = np.array(label).flatten()  # Ensure label is 1D.\n",
    "    if flat_label.size == 2:\n",
    "        valence, arousal = flat_label\n",
    "    elif flat_label.size >= 3:\n",
    "        valence, arousal = flat_label[1], flat_label[2]\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    if valence < 0 and arousal < 0:\n",
    "        return \"Low valence, Low arousal\"\n",
    "    elif valence < 0 and arousal >= 0:\n",
    "        return \"Low valence, High arousal\"\n",
    "    elif valence >= 0 and arousal < 0:\n",
    "        return \"High valence, Low arousal\"\n",
    "    else:\n",
    "        return \"High valence, High arousal\"\n",
    "\n",
    "# --- Feature Extraction Functions ---\n",
    "def extract_features(signals, fs=128):\n",
    "    \"\"\"\n",
    "    Extract features for multiple signals (ECG, GSR, EEG) from a dictionary.\n",
    "    If advanced processing (e.g., HRV from ECG) fails, falls back to basic statistics.\n",
    "    \"\"\"\n",
    "    feat_list = []\n",
    "\n",
    "    # ---------- ECG Features ----------\n",
    "    if 'ecg' in signals:\n",
    "        ecg_signal = np.array(signals['ecg']).flatten()\n",
    "        if len(ecg_signal) <= 18:\n",
    "            ecg_feats = [0.0] * 10  # Not enough data for advanced features.\n",
    "        else:\n",
    "            try:\n",
    "                ecg_cleaned = nk.ecg_clean(ecg_signal, sampling_rate=fs)\n",
    "                _, rpeaks = nk.ecg_peaks(ecg_cleaned, sampling_rate=fs)\n",
    "                # Check if any R-peaks were detected\n",
    "                if len(rpeaks.get(\"ECG_R_Peaks\", [])) == 0:\n",
    "                    raise ValueError(\"No R-peaks detected.\")\n",
    "                hrv = nk.hrv(rpeaks, sampling_rate=fs, show=False)\n",
    "                feature_names = [\n",
    "                    \"RMSSD\", \"SDNN\", \"pNN50\", \"pNN20\",\n",
    "                    \"LF/HF\", \"HF\", \"LF\", \"VLF\", \"HRV_TI\", \"SDSD\"\n",
    "                ]\n",
    "                ecg_feats = []\n",
    "                for name in feature_names:\n",
    "                    if name in hrv.columns and not np.isnan(hrv[name].values[0]):\n",
    "                        ecg_feats.append(hrv[name].values[0])\n",
    "                    else:\n",
    "                        ecg_feats.append(0.0)\n",
    "            except Exception as e:\n",
    "                # Fallback: Compute basic statistics if advanced features fail.\n",
    "                basic_stats = [\n",
    "                    np.mean(ecg_signal),\n",
    "                    np.std(ecg_signal),\n",
    "                    np.min(ecg_signal),\n",
    "                    np.max(ecg_signal),\n",
    "                    np.median(ecg_signal)\n",
    "                ]\n",
    "                # Pad to reach length 10.\n",
    "                ecg_feats = basic_stats + [0.0] * (10 - len(basic_stats))\n",
    "        feat_list.append(np.array(ecg_feats))\n",
    "\n",
    "    # ---------- GSR Features ----------\n",
    "    if 'gsr' in signals:\n",
    "        gsr_signal = np.array(signals['gsr']).flatten()\n",
    "        if len(gsr_signal) > 2:\n",
    "            try:\n",
    "                eda_cleaned = nk.eda_clean(gsr_signal, sampling_rate=fs)\n",
    "                eda_peaks, _ = nk.eda_peaks(eda_cleaned, sampling_rate=fs)\n",
    "                num_scr_peaks = eda_peaks.get(\"SCR_Peaks\", np.array([0])).sum()\n",
    "            except Exception:\n",
    "                num_scr_peaks = 0.0\n",
    "            gsr_feats = [\n",
    "                np.mean(gsr_signal),\n",
    "                np.std(gsr_signal),\n",
    "                np.min(gsr_signal),\n",
    "                np.max(gsr_signal),\n",
    "                kurtosis(gsr_signal),\n",
    "                skew(gsr_signal),\n",
    "                num_scr_peaks\n",
    "            ]\n",
    "        else:\n",
    "            gsr_feats = [0.0] * 7\n",
    "        feat_list.append(np.array(gsr_feats))\n",
    "\n",
    "    # ---------- EEG Features ----------\n",
    "    if 'eeg' in signals:\n",
    "        eeg_data = np.array(signals['eeg'])\n",
    "        if eeg_data.ndim == 1:\n",
    "            eeg_data = eeg_data[None, :]  # Ensure 2D shape.\n",
    "        all_channels_feats = []\n",
    "        for ch in range(eeg_data.shape[0]):\n",
    "            channel_signal = eeg_data[ch, :]\n",
    "            if len(channel_signal) < 2:\n",
    "                ch_feats = [0.0] * 6\n",
    "            else:\n",
    "                activity = np.var(channel_signal)\n",
    "                mobility = np.std(np.diff(channel_signal)) / (np.std(channel_signal) + 1e-8)\n",
    "                diff_signal = np.diff(channel_signal)\n",
    "                complexity = (np.std(np.diff(diff_signal)) / (np.std(diff_signal) + 1e-8)) / (mobility + 1e-8)\n",
    "                freqs, psd = welch(channel_signal, fs=fs, nperseg=min(256, len(channel_signal)))\n",
    "                def bandpower(f, pxx, fmin, fmax):\n",
    "                    idx = np.logical_and(f >= fmin, f <= fmax)\n",
    "                    # Use trapezoid integration as recommended.\n",
    "                    return np.trapezoid(pxx[idx], x=f[idx])\n",
    "                alpha = bandpower(freqs, psd, 8, 14)\n",
    "                beta  = bandpower(freqs, psd, 14, 30)\n",
    "                gamma = bandpower(freqs, psd, 30, 50)\n",
    "                ch_feats = [activity, mobility, complexity, alpha, beta, gamma]\n",
    "            all_channels_feats.append(ch_feats)\n",
    "        # Average across channels.\n",
    "        eeg_feats = np.mean(all_channels_feats, axis=0)\n",
    "        feat_list.append(eeg_feats)\n",
    "\n",
    "    # ---------- Combine all features ----------\n",
    "    if len(feat_list) == 0:\n",
    "        return np.zeros(10)\n",
    "    return np.concatenate(feat_list)\n",
    "\n",
    "def build_dataset(joined_data, labels_array, target_length=None):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    n_trials = joined_data.shape[1]\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        trial_data = joined_data[0, i]\n",
    "        signal = process_trial_signal(trial_data, target_length)\n",
    "        if signal.size == 0 or signal.shape[0] == 0:\n",
    "            print(f\"Warning: Trial {i} has an empty signal. Skipping trial.\")\n",
    "            continue\n",
    "\n",
    "        signals_dict = split_into_modalities(signal)\n",
    "        feats = extract_features(signals_dict, fs=128)\n",
    "        \n",
    "        lbl = np.array(labels_array[0, i]).squeeze()\n",
    "        if lbl.size < 3:\n",
    "            print(f\"Warning: Trial {i} does not have enough label data. Skipping trial.\")\n",
    "            continue\n",
    "        selected_label = lbl[1:3]  # use only the second and third columns\n",
    "        discrete_label = discretize_label(selected_label)\n",
    "\n",
    "        X_list.append(feats)\n",
    "        y_list.append(discrete_label)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return None, None\n",
    "\n",
    "    X_array = np.vstack(X_list)\n",
    "    y_array = np.array(y_list)\n",
    "    return X_array, y_array\n",
    "\n",
    "# Load raw signals for CNN/LSTM/GRU\n",
    "def load_all_patients_raw_signal(num_patients=40, target_length=None):\n",
    "    X_list, y_list = [], []\n",
    "    for patient in range(1, num_patients + 1):\n",
    "        data = load_patient_preprocessed_data(patient)\n",
    "        joined_data = data['joined_data']\n",
    "        labels_array = data['labels_ext_annotation']\n",
    "        \n",
    "        X_patient, y_patient = build_dataset(joined_data, labels_array, target_length=target_length)\n",
    "        \n",
    "        if X_patient is not None:\n",
    "            X_list.append(X_patient)\n",
    "            y_list.append(y_patient)\n",
    "\n",
    "    if not X_list:\n",
    "        raise ValueError(\"No data loaded!\")\n",
    "\n",
    "    X_all = np.vstack(X_list)\n",
    "    y_all = np.concatenate(y_list)\n",
    "\n",
    "    return X_all, y_all\n",
    "\n",
    "def pad_trials(trials, pad_mode='constant', constant_values=0):\n",
    "    \"\"\"\n",
    "    Given a list of 2D arrays (each with shape (channels, time)), pad them so that all have the same shape.\n",
    "    Both channel and time dimensions are padded using constant values.\n",
    "    \"\"\"\n",
    "    # Determine maximum dimensions among all trials.\n",
    "    max_channels = max(trial.shape[0] for trial in trials)\n",
    "    max_time = max(trial.shape[1] for trial in trials)\n",
    "    \n",
    "    padded_trials = []\n",
    "    for trial in trials:\n",
    "        ch, t = trial.shape\n",
    "        # Pad channels if needed.\n",
    "        if ch < max_channels:\n",
    "            trial = np.pad(trial, ((0, max_channels - ch), (0, 0)), mode=pad_mode, constant_values=constant_values)\n",
    "        # Pad time dimension if needed.\n",
    "        if t < max_time:\n",
    "            trial = np.pad(trial, ((0, 0), (0, max_time - t)), mode=pad_mode, constant_values=constant_values)\n",
    "        elif t > max_time:\n",
    "            trial = trial[:, :max_time]\n",
    "        padded_trials.append(trial)\n",
    "    return np.stack(padded_trials, axis=0)\n",
    "\n",
    "def get_trial_lengths(num_patients=40):\n",
    "    patient_lengths = {}\n",
    "    for patient in range(1, num_patients + 1):\n",
    "        print(f\"\\nPatient {patient}:\")\n",
    "        data = load_patient_preprocessed_data(patient)\n",
    "        joined_data = data['joined_data']\n",
    "        lengths = []\n",
    "        n_trials = joined_data.shape[1]\n",
    "        for i in range(n_trials):\n",
    "            trial_data = joined_data[0, i]\n",
    "            # Process without fixed target_length so we keep original lengths.\n",
    "            signal = process_trial_signal(trial_data, target_length=None)\n",
    "            if signal.size == 0 or signal.shape[1] == 0:\n",
    "                print(f\"  Trial {i}: empty signal\")\n",
    "                continue\n",
    "            # signal is a 2D array: [channels, time]\n",
    "            trial_length = signal.shape[1]\n",
    "            lengths.append(trial_length)\n",
    "            print(f\"  Trial {i}: length = {trial_length}\")\n",
    "        patient_lengths[patient] = lengths\n",
    "    return patient_lengths\n",
    "\n",
    "def load_all_patients_raw_signal_deep_chunked(num_patients=40, target_length=None):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    for patient in range(1, num_patients+1):\n",
    "        print(f\"Processing patient {patient}...\")\n",
    "        data = load_patient_preprocessed_data(patient)\n",
    "        joined_data = data['joined_data']\n",
    "        labels_array = data['labels_ext_annotation']\n",
    "        n_trials = joined_data.shape[1]\n",
    "        for i in range(n_trials):\n",
    "            # Process label first.\n",
    "            lbl = np.array(labels_array[0, i]).squeeze()\n",
    "            if lbl.size == 0:\n",
    "                print(f\"Warning: Patient {patient} Trial {i} has empty label. Skipping trial.\")\n",
    "                continue\n",
    "            if lbl.ndim == 2:\n",
    "                lbl_processed = np.mean(lbl, axis=0)\n",
    "            elif lbl.ndim == 1:\n",
    "                lbl_processed = lbl\n",
    "            else:\n",
    "                lbl_processed = lbl.flatten()[0]\n",
    "            discrete_label = discretize_label(lbl_processed)\n",
    "            \n",
    "            trial_data = joined_data[0, i]\n",
    "            # Process trial signal to a 2D array with fixed target_length.\n",
    "            signal = process_trial_signal(trial_data, target_length).astype(np.float32)\n",
    "            all_X.append(signal)\n",
    "            all_y.append(discrete_label)\n",
    "        \n",
    "        del data, joined_data, labels_array\n",
    "        gc.collect()\n",
    "    \n",
    "    if len(all_X) == 0:\n",
    "        raise ValueError(\"No patient data loaded.\")\n",
    "    \n",
    "    if target_length is None:\n",
    "        X_all = pad_trials(all_X, pad_mode='constant', constant_values=0)\n",
    "    else:\n",
    "        # Pad the channel dimension so that all trials have the same number of channels.\n",
    "        max_channels = max(trial.shape[0] for trial in all_X)\n",
    "        padded_trials = []\n",
    "        for trial in all_X:\n",
    "            channels_to_pad = max_channels - trial.shape[0]\n",
    "            if channels_to_pad > 0:\n",
    "                trial = np.pad(trial, ((0, channels_to_pad), (0, 0)), mode='constant', constant_values=0)\n",
    "            padded_trials.append(trial)\n",
    "        X_all = np.stack(padded_trials, axis=0)\n",
    "    \n",
    "    y_all = np.array(all_y)\n",
    "    return X_all, y_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patient 1...\n",
      "Processing patient 2...\n",
      "Processing patient 3...\n",
      "Processing patient 4...\n",
      "Processing patient 5...\n",
      "Processing patient 6...\n",
      "Processing patient 7...\n",
      "Processing patient 8...\n",
      "Warning: Patient 8 Trial 0 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 1 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 2 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 3 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 4 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 5 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 6 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 7 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 8 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 9 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 10 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 11 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 12 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 13 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 14 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 15 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 16 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 17 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 18 has empty label. Skipping trial.\n",
      "Warning: Patient 8 Trial 19 has empty label. Skipping trial.\n",
      "Processing patient 9...\n",
      "Processing patient 10...\n",
      "Processing patient 11...\n",
      "Processing patient 12...\n",
      "Processing patient 13...\n",
      "Processing patient 14...\n",
      "Processing patient 15...\n",
      "Processing patient 16...\n",
      "Processing patient 17...\n",
      "Processing patient 18...\n",
      "Processing patient 19...\n",
      "Processing patient 20...\n",
      "Processing patient 21...\n",
      "Processing patient 22...\n",
      "Processing patient 23...\n",
      "Processing patient 24...\n",
      "Warning: Patient 24 Trial 0 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 1 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 2 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 3 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 4 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 5 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 6 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 7 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 8 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 9 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 10 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 11 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 12 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 13 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 14 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 15 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 16 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 17 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 18 has empty label. Skipping trial.\n",
      "Warning: Patient 24 Trial 19 has empty label. Skipping trial.\n",
      "Processing patient 25...\n",
      "Processing patient 26...\n",
      "Processing patient 27...\n",
      "Processing patient 28...\n",
      "Warning: Patient 28 Trial 0 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 1 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 2 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 3 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 4 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 5 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 6 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 7 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 8 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 9 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 10 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 11 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 12 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 13 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 14 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 15 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 16 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 17 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 18 has empty label. Skipping trial.\n",
      "Warning: Patient 28 Trial 19 has empty label. Skipping trial.\n",
      "Processing patient 29...\n",
      "Processing patient 30...\n",
      "Processing patient 31...\n",
      "Processing patient 32...\n",
      "Processing patient 33...\n",
      "Processing patient 34...\n",
      "Processing patient 35...\n",
      "Processing patient 36...\n",
      "Processing patient 37...\n",
      "Processing patient 38...\n",
      "Processing patient 39...\n",
      "Processing patient 40...\n",
      "Input shape: (592, 5, 181802)\n",
      "Number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "# Get and print trial lengths for each patient.\n",
    "# trial_lengths = get_trial_lengths(num_patients=40)\n",
    "\n",
    "# For deep learning we want a fixed target length.\n",
    "target_length = 5 \n",
    "\n",
    "# Note: load_all_patients_raw_signal_deep_chunked loops over patient numbers starting at 1.\n",
    "X_raw, y_raw = load_all_patients_raw_signal_deep_chunked(num_patients=40, target_length=target_length)\n",
    "# X_raw has shape (n_trials, channels, target_length)\n",
    "\n",
    "# For deep models (Conv1D), we interpret the time dimension as the sequence length.\n",
    "# Rearrange the input to shape (n_trials, target_length, channels)\n",
    "X_dl = np.transpose(X_raw, (0, 2, 1))\n",
    "\n",
    "# Encode string labels into integers and then one-hot vectors.\n",
    "unique_labels = np.unique(y_raw)\n",
    "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_int = np.array([label_to_int[label] for label in y_raw])\n",
    "y_cat = to_categorical(y_int)\n",
    "\n",
    "# Split into training and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_dl, y_cat, test_size=0.2, random_state=42, stratify=y_cat\n",
    ")\n",
    "\n",
    "# Get input shape details.\n",
    "timesteps = X_train.shape[1]   # target_length\n",
    "num_channels = X_train.shape[2]\n",
    "num_classes = y_cat.shape[1]\n",
    "\n",
    "print(\"Input shape:\", X_train.shape)\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Pure CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">17,453,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │    \u001b[38;5;34m17,453,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m6,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,568,004</span> (67.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,568,004\u001b[0m (67.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,566,852</span> (67.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,566,852\u001b[0m (67.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> (4.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,152\u001b[0m (4.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - accuracy: 0.6294 - loss: 1.4633 - val_accuracy: 0.6555 - val_loss: 1.2263 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 213ms/step - accuracy: 0.6546 - loss: 1.1847 - val_accuracy: 0.6555 - val_loss: 1.0833 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - accuracy: 0.6773 - loss: 1.0384 - val_accuracy: 0.6555 - val_loss: 0.9935 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 203ms/step - accuracy: 0.6914 - loss: 0.9383 - val_accuracy: 0.6555 - val_loss: 0.9652 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 205ms/step - accuracy: 0.6947 - loss: 0.8872 - val_accuracy: 0.6555 - val_loss: 0.9566 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 200ms/step - accuracy: 0.6730 - loss: 0.9310 - val_accuracy: 0.6555 - val_loss: 0.9531 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.6672 - loss: 0.9182 - val_accuracy: 0.6555 - val_loss: 0.9521 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step - accuracy: 0.6413 - loss: 0.9875 - val_accuracy: 0.6555 - val_loss: 0.9518 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 0.6703 - loss: 0.9456 - val_accuracy: 0.6555 - val_loss: 0.9519 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.6845 - loss: 0.9245 - val_accuracy: 0.6555 - val_loss: 0.9515 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.6816 - loss: 0.9071 - val_accuracy: 0.6555 - val_loss: 0.9509 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.6886 - loss: 0.8962 - val_accuracy: 0.6555 - val_loss: 0.9510 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.7165 - loss: 0.8545 - val_accuracy: 0.6555 - val_loss: 0.9515 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.6426 - loss: 0.9702 - val_accuracy: 0.6555 - val_loss: 0.9507 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 0.7076 - loss: 0.8493 - val_accuracy: 0.6555 - val_loss: 0.9517 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step - accuracy: 0.6768 - loss: 0.9189 - val_accuracy: 0.6555 - val_loss: 0.9514 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.6647 - loss: 0.9245 - val_accuracy: 0.6555 - val_loss: 0.9511 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.6856 - loss: 0.8849 - val_accuracy: 0.6555 - val_loss: 0.9518 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.6724 - loss: 0.9038 - val_accuracy: 0.6555 - val_loss: 0.9510 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 203ms/step - accuracy: 0.6932 - loss: 0.8760 - val_accuracy: 0.6555 - val_loss: 0.9512 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 203ms/step - accuracy: 0.6508 - loss: 0.9317 - val_accuracy: 0.6555 - val_loss: 0.9510 - learning_rate: 5.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 199ms/step - accuracy: 0.6548 - loss: 0.9411 - val_accuracy: 0.6555 - val_loss: 0.9510 - learning_rate: 5.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 199ms/step - accuracy: 0.6749 - loss: 0.9076 - val_accuracy: 0.6555 - val_loss: 0.9519 - learning_rate: 5.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.6645 - loss: 0.9655 - val_accuracy: 0.6555 - val_loss: 0.9514 - learning_rate: 5.0000e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6462 - loss: 0.9841\n",
      "CNN Test Accuracy: 0.6689189076423645\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model_cnn = Sequential([\n",
    "    # First convolutional block\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(timesteps, num_channels), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Second convolutional block\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Third convolutional block\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    # Replace MaxPooling1D with GlobalMaxPooling1D to avoid reducing the dimension below 1\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Classification block\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_cnn.summary()\n",
    "\n",
    "# Callbacks: Early stopping and learning rate reduction\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history_cnn = model_cnn.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=30, \n",
    "    batch_size=16, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_cnn, acc_cnn = model_cnn.evaluate(X_test, y_test)\n",
    "print(\"CNN Test Accuracy:\", acc_cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: RNN with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">93,115,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │    \u001b[38;5;34m93,115,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,175,428</span> (355.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,175,428\u001b[0m (355.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,174,788</span> (355.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,174,788\u001b[0m (355.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> (2.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m640\u001b[0m (2.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 914ms/step - accuracy: 0.4557 - loss: 1.8267 - val_accuracy: 0.6555 - val_loss: 1.2343 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 875ms/step - accuracy: 0.6715 - loss: 1.1905 - val_accuracy: 0.6555 - val_loss: 1.1157 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 872ms/step - accuracy: 0.6804 - loss: 1.0618 - val_accuracy: 0.6555 - val_loss: 1.0308 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 866ms/step - accuracy: 0.6627 - loss: 1.0067 - val_accuracy: 0.6555 - val_loss: 0.9864 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 867ms/step - accuracy: 0.6525 - loss: 0.9788 - val_accuracy: 0.6555 - val_loss: 0.9657 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 882ms/step - accuracy: 0.6968 - loss: 0.8925 - val_accuracy: 0.6555 - val_loss: 0.9585 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 862ms/step - accuracy: 0.6808 - loss: 0.8985 - val_accuracy: 0.6555 - val_loss: 0.9545 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 865ms/step - accuracy: 0.7014 - loss: 0.8691 - val_accuracy: 0.6555 - val_loss: 0.9529 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 863ms/step - accuracy: 0.6403 - loss: 0.9580 - val_accuracy: 0.6555 - val_loss: 0.9515 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 859ms/step - accuracy: 0.6898 - loss: 0.8776 - val_accuracy: 0.6555 - val_loss: 0.9519 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 866ms/step - accuracy: 0.6647 - loss: 0.9279 - val_accuracy: 0.6555 - val_loss: 0.9511 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 860ms/step - accuracy: 0.7135 - loss: 0.8703 - val_accuracy: 0.6555 - val_loss: 0.9511 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 864ms/step - accuracy: 0.7032 - loss: 0.8768 - val_accuracy: 0.6555 - val_loss: 0.9505 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 874ms/step - accuracy: 0.6640 - loss: 0.9346 - val_accuracy: 0.6555 - val_loss: 0.9503 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 867ms/step - accuracy: 0.7095 - loss: 0.8571 - val_accuracy: 0.6555 - val_loss: 0.9522 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 863ms/step - accuracy: 0.6549 - loss: 0.9264 - val_accuracy: 0.6555 - val_loss: 0.9508 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 861ms/step - accuracy: 0.6780 - loss: 0.9264 - val_accuracy: 0.6555 - val_loss: 0.9511 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 860ms/step - accuracy: 0.6673 - loss: 0.9169 - val_accuracy: 0.6555 - val_loss: 0.9510 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 860ms/step - accuracy: 0.6744 - loss: 0.8939 - val_accuracy: 0.6555 - val_loss: 0.9515 - learning_rate: 5.0000e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy: 0.6462 - loss: 0.9827\n",
      "RNN Test Accuracy: 0.6689189076423645\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Build an improved RNN model with stacked bidirectional LSTMs\n",
    "model_rnn = Sequential([\n",
    "    # First Bidirectional LSTM layer returns sequences\n",
    "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(timesteps, num_channels)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Second LSTM layer (can be unidirectional now)\n",
    "    LSTM(64, return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Fully connected classification block\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_rnn.summary()\n",
    "\n",
    "# Callbacks: Early stopping and learning rate reduction\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history_rnn = model_rnn.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=30, \n",
    "    batch_size=16,\n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_rnn, acc_rnn = model_rnn.evaluate(X_test, y_test)\n",
    "print(\"RNN Test Accuracy:\", acc_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">69,837,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │    \u001b[38;5;34m69,837,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m37,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,884,676</span> (266.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m69,884,676\u001b[0m (266.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,884,036</span> (266.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m69,884,036\u001b[0m (266.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> (2.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m640\u001b[0m (2.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 730ms/step - accuracy: 0.4826 - loss: 1.8101 - val_accuracy: 0.6555 - val_loss: 1.2291 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 689ms/step - accuracy: 0.7083 - loss: 1.1875 - val_accuracy: 0.6555 - val_loss: 1.1181 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 682ms/step - accuracy: 0.6724 - loss: 1.0950 - val_accuracy: 0.6555 - val_loss: 1.0382 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 678ms/step - accuracy: 0.6832 - loss: 0.9883 - val_accuracy: 0.6555 - val_loss: 0.9888 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 680ms/step - accuracy: 0.7164 - loss: 0.9117 - val_accuracy: 0.6555 - val_loss: 0.9685 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 688ms/step - accuracy: 0.6759 - loss: 0.9514 - val_accuracy: 0.6555 - val_loss: 0.9598 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 684ms/step - accuracy: 0.6942 - loss: 0.8847 - val_accuracy: 0.6555 - val_loss: 0.9561 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 685ms/step - accuracy: 0.6691 - loss: 0.9500 - val_accuracy: 0.6555 - val_loss: 0.9534 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 680ms/step - accuracy: 0.6634 - loss: 0.9326 - val_accuracy: 0.6555 - val_loss: 0.9523 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 679ms/step - accuracy: 0.6746 - loss: 0.8973 - val_accuracy: 0.6555 - val_loss: 0.9530 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 680ms/step - accuracy: 0.6848 - loss: 0.8888 - val_accuracy: 0.6555 - val_loss: 0.9522 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 695ms/step - accuracy: 0.6906 - loss: 0.8926 - val_accuracy: 0.6555 - val_loss: 0.9514 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 686ms/step - accuracy: 0.6831 - loss: 0.9121 - val_accuracy: 0.6555 - val_loss: 0.9506 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 688ms/step - accuracy: 0.6570 - loss: 0.9253 - val_accuracy: 0.6555 - val_loss: 0.9509 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 697ms/step - accuracy: 0.6581 - loss: 0.9358 - val_accuracy: 0.6555 - val_loss: 0.9513 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 692ms/step - accuracy: 0.6555 - loss: 0.9557 - val_accuracy: 0.6555 - val_loss: 0.9515 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 692ms/step - accuracy: 0.6630 - loss: 0.9221 - val_accuracy: 0.6555 - val_loss: 0.9513 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 692ms/step - accuracy: 0.6533 - loss: 0.9636 - val_accuracy: 0.6555 - val_loss: 0.9518 - learning_rate: 5.0000e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy: 0.6462 - loss: 0.9830\n",
      "RNN+GRU Test Accuracy: 0.6689189076423645\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Build an improved GRU-based RNN model\n",
    "model_rnn_gru = Sequential([\n",
    "    # First Bidirectional GRU layer to capture both forward and backward temporal patterns.\n",
    "    Bidirectional(GRU(64, return_sequences=True), input_shape=(timesteps, num_channels)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Second GRU layer for further temporal feature extraction.\n",
    "    GRU(64, return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Fully connected block for classification.\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_rnn_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_rnn_gru.summary()\n",
    "\n",
    "# Define callbacks: Early stopping and learning rate reduction\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history_rnn_gru = model_rnn_gru.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_gru, acc_gru = model_rnn_gru.evaluate(X_test, y_test)\n",
    "print(\"RNN+GRU Test Accuracy:\", acc_gru)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
