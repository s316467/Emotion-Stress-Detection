{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "import scipy.io\n",
    "import scipy.signal as sgl\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, balanced_accuracy_score # Added balanced_accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, LSTM, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "import neurokit2 as nk\n",
    "# from imblearn.over_sampling import SMOTE # Not using SMOTE currently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants for Windowing ---\n",
    "FS = 128\n",
    "WINDOW_SEC = 2 # Try 2 seconds first\n",
    "STEP_SEC = 1   # Correspondingly smaller step (e.g., 50% overlap)\n",
    "WINDOW_SAMPLES = int(WINDOW_SEC * FS) # Will be 256\n",
    "STEP_SAMPLES = int(STEP_SEC * FS)     # Will be 128\n",
    "\n",
    "\n",
    "# --- Data Loading Function ---\n",
    "def load_patient_preprocessed_data(patient_number):\n",
    "    # Keep this function as is\n",
    "    base_dir = r\"C:\\Users\\ferri\\Downloads\\PoliTO\\Tesi\\DSs\\Emotion-Stress\\AMIGOS\"\n",
    "    file_path = os.path.join(\n",
    "        base_dir, \"Data preprocessed\",\n",
    "        f\"Data_Preprocessed_P{patient_number:02d}\",\n",
    "        f\"Data_Preprocessed_P{patient_number:02d}.mat\"\n",
    "    )\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    return data\n",
    "\n",
    "# --- Preprocessing functions for pipeline ---\n",
    "def process_trial_signal(signal, target_length=None, fs=512):\n",
    "    \"\"\"\n",
    "    Convert a trial's raw signal into a 2D array [channels, time].\n",
    "    Applies transpose fix, downsampling, filtering, baseline removal.\n",
    "    \"\"\"\n",
    "    # Convert to float32.\n",
    "    signal = np.array(signal, dtype=np.float32)\n",
    "\n",
    "    # --- ADD TRANSPOSE STEP ---\n",
    "    # Check if dimensions look like (samples, channels) and transpose if needed\n",
    "    if signal.ndim == 2 and signal.shape[0] > signal.shape[1]:\n",
    "         # Heuristic: If rows > columns, assume it's (samples, channels)\n",
    "         print(f\"    Transposing input signal from {signal.shape} to {signal.T.shape}\") # Add print\n",
    "         signal = signal.T # Transpose to (channels, samples)\n",
    "    # --- END TRANSPOSE STEP ---\n",
    "\n",
    "    # Check if signal is empty AFTER potential transpose\n",
    "    if signal.size == 0:\n",
    "        return np.empty((0,0), dtype=np.float32)\n",
    "\n",
    "    # If the signal is 1D, reshape to (1, length).\n",
    "    # This should happen AFTER transpose if original was (L, 1)\n",
    "    if signal.ndim == 1:\n",
    "        signal = signal[None, :] # Now shape (1, L)\n",
    "\n",
    "    # Downsampling parameters.\n",
    "    N = 4\n",
    "    lowcut, highcut = 1.0, 45.0\n",
    "    desired_fs = 128 # This is the FS used for feature extraction later\n",
    "    down_factor = fs // desired_fs\n",
    "\n",
    "    # 1) Downsample each channel. (Now iterates correctly over channels)\n",
    "    downsampled = []\n",
    "    for ch_data in signal: # ch_data should now be one channel's time series\n",
    "        if ch_data.size == 0: continue\n",
    "        ch_data_down = ch_data[::down_factor]\n",
    "        downsampled.append(ch_data_down)\n",
    "\n",
    "    # If no channels had data, return an empty array.\n",
    "    if not downsampled:\n",
    "        return np.empty((0,0), dtype=np.float32)\n",
    "\n",
    "    # Stack downsampled channels. Pad if necessary (though simple slicing should yield same length)\n",
    "    try:\n",
    "        max_len_down = max(len(ch) for ch in downsampled)\n",
    "        padded_downsampled = []\n",
    "        for ch in downsampled:\n",
    "             pad_width = max_len_down - len(ch)\n",
    "             if pad_width > 0:\n",
    "                  padded_ch = np.pad(ch, (0, pad_width), mode='edge')\n",
    "                  padded_downsampled.append(padded_ch)\n",
    "             else:\n",
    "                  padded_downsampled.append(ch)\n",
    "        signal = np.vstack([ch[None, :] for ch in padded_downsampled])\n",
    "    except ValueError as e:\n",
    "         print(f\"Error during vstack after downsampling: {e}. Lengths: {[len(ch) for ch in downsampled]}\")\n",
    "         return np.empty((0,0), dtype=np.float32) # Return empty on error\n",
    "\n",
    "    # 2) Bandpass filter design.\n",
    "    nyquist = 0.5 * desired_fs\n",
    "    b, a = sgl.butter(N=4, Wn=[lowcut/nyquist, highcut/nyquist], btype='band')\n",
    "    min_len_filt = 3 * (max(len(a), len(b)) - 1) # Renamed variable\n",
    "\n",
    "    # Filter each channel; if too short, skip filtering.\n",
    "    filtered = []\n",
    "    for ch_data in signal:\n",
    "        if len(ch_data) < min_len_filt:\n",
    "            ch_data_filt = ch_data  # Fallback: leave unfiltered.\n",
    "        else:\n",
    "            try:\n",
    "                 ch_data_filt = sgl.filtfilt(b, a, ch_data)\n",
    "            except ValueError as e: # Add specific error handling for filtfilt\n",
    "                 print(f\"    filtfilt error: {e} on data length {len(ch_data)}. Skipping filter.\")\n",
    "                 ch_data_filt = ch_data # Fallback if filtfilt fails\n",
    "        filtered.append(ch_data_filt)\n",
    "    signal = np.vstack([ch[None, :] for ch in filtered])\n",
    "\n",
    "    # 3) Baseline removal (subtract mean from each channel).\n",
    "    baseline_removed = []\n",
    "    for ch_data in signal:\n",
    "        mean_val = np.mean(ch_data) if ch_data.size > 0 else 0\n",
    "        ch_data_bs = ch_data - mean_val\n",
    "        baseline_removed.append(ch_data_bs)\n",
    "    signal = np.vstack([ch[None, :] for ch in baseline_removed])\n",
    "\n",
    "    # 4) Padding/Truncation - Keep disabled if target_length is None\n",
    "    if target_length is not None:\n",
    "        processed = []\n",
    "        for ch_data in signal:\n",
    "            ch_len = len(ch_data)\n",
    "            if ch_len == 0:\n",
    "                proc = np.zeros(target_length, dtype=np.float32)\n",
    "            elif ch_len < target_length:\n",
    "                pad_width = target_length - ch_len\n",
    "                proc = np.pad(ch_data, (0, pad_width), mode='edge')\n",
    "            else:\n",
    "                proc = ch_data[:target_length]\n",
    "            processed.append(proc.astype(np.float32))\n",
    "        signal = np.vstack([p[None, :] for p in processed])\n",
    "\n",
    "    return signal.astype(np.float32)\n",
    "\n",
    "\n",
    "def split_into_modalities(signal_window):\n",
    "    # Keep this function as is, it works on a window [channels, time]\n",
    "    # If the signal is 1D, assume it represents a single modality (e.g., ECG).\n",
    "    if signal_window.ndim == 1:\n",
    "        # Reshape if 1D (e.g., single channel passed)\n",
    "        signal_window = signal_window[None, :]\n",
    "        if signal_window.shape[0] == 1: # Assume it's ECG if only one channel\n",
    "            return {\"ecg\": signal_window.flatten()}\n",
    "        else: # Should not happen if input is always [channels, time]\n",
    "             return {}\n",
    "\n",
    "    elif signal_window.ndim == 2:\n",
    "        # If multi-channel, split into ECG, GSR, and EEG as desired.\n",
    "        # Adjust indices based on your actual channel order in 'joined_data'\n",
    "        # Assuming: 0=ECG, 1=GSR, 2=EEG (as per your original code)\n",
    "        modalities = {}\n",
    "        if signal_window.shape[0] > 0:\n",
    "            modalities[\"ecg\"] = signal_window[0, :]\n",
    "        if signal_window.shape[0] > 1:\n",
    "            modalities[\"gsr\"] = signal_window[1, :]\n",
    "        if signal_window.shape[0] > 2:\n",
    "             # Assuming EEG is just the 3rd channel for simplicity here\n",
    "             # If multiple EEG channels, you'd handle them differently in extract_features\n",
    "            modalities[\"eeg\"] = signal_window[2, :]\n",
    "        return modalities\n",
    "    else:\n",
    "        # Handle unexpected dimensions\n",
    "        print(f\"Warning: Unexpected signal dimension {signal_window.ndim}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def discretize_label(label):\n",
    "    # Keep this function as is\n",
    "    flat_label = np.array(label).flatten()  # Ensure label is 1D.\n",
    "    if flat_label.size == 2:\n",
    "        valence, arousal = flat_label\n",
    "    elif flat_label.size >= 3:\n",
    "        valence, arousal = flat_label[1], flat_label[2]\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    if valence < 0 and arousal < 0:\n",
    "        return \"Low valence, Low arousal\"\n",
    "    elif valence < 0 and arousal >= 0:\n",
    "        return \"Low valence, High arousal\"\n",
    "    elif valence >= 0 and arousal < 0:\n",
    "        return \"High valence, Low arousal\"\n",
    "    else:\n",
    "        return \"High valence, High arousal\"\n",
    "\n",
    "# --- Feature Extraction Functions ---\n",
    "def extract_features(signals_dict, fs=FS): # Use FS=128 defined above\n",
    "    # Keep this function as is, it extracts features from a dictionary of signals (now windowed signals)\n",
    "    \"\"\"\n",
    "    Extract features for multiple signals (ECG, GSR, EEG) from a dictionary.\n",
    "    If advanced processing (e.g., HRV from ECG) fails, falls back to basic statistics.\n",
    "    **Consider adding more advanced features here later.**\n",
    "    \"\"\"\n",
    "    feat_list = []\n",
    "    # --- ECG Features (Consider adding more HRV: nk.hrv_frequency, nk.hrv_nonlinear) ---\n",
    "    if 'ecg' in signals_dict:\n",
    "        ecg_signal = np.array(signals_dict['ecg']).flatten()\n",
    "        # Check length against minimum needed by neurokit processing steps\n",
    "        min_len_nk = 30 # Arbitrary minimum, adjust based on nk requirements for short signals\n",
    "        if len(ecg_signal) < min_len_nk:\n",
    "             # Fallback for very short windows\n",
    "            basic_stats = [np.mean(ecg_signal), np.std(ecg_signal), np.min(ecg_signal), np.max(ecg_signal), np.median(ecg_signal)]\n",
    "            ecg_feats = basic_stats + [0.0] * (10 - len(basic_stats)) # Pad to expected length\n",
    "        else:\n",
    "            try:\n",
    "                ecg_cleaned = nk.ecg_clean(ecg_signal, sampling_rate=fs)\n",
    "                _, rpeaks = nk.ecg_peaks(ecg_cleaned, sampling_rate=fs, correct_artifacts=True) # Added correct_artifacts\n",
    "                # Check if enough R-peaks were detected for HRV analysis\n",
    "                if len(rpeaks.get(\"ECG_R_Peaks\", [])) < 3: # Need at least a few peaks for HRV\n",
    "                    raise ValueError(\"Not enough R-peaks detected for HRV.\")\n",
    "                hrv = nk.hrv(rpeaks, sampling_rate=fs, show=False)\n",
    "                # Select a consistent set of HRV features (adjust as needed)\n",
    "                feature_names = [\n",
    "                    \"RMSSD\", \"SDNN\", \"pNN50\", \"pNN20\",\n",
    "                    \"LFHF\", # Use LFHF instead of LF/HF if available\n",
    "                    \"HF\", \"LF\", \"VLF\", \"SDSD\", \"IQRNN\" # Replaced HRV_TI with IQRNN\n",
    "                 ]\n",
    "                ecg_feats = []\n",
    "                # Check HRV columns, handle potential missing columns or NaNs\n",
    "                hrv_cols = hrv.columns\n",
    "                for name in feature_names:\n",
    "                     # Handle different naming conventions (e.g., LF/HF vs LFHF)\n",
    "                    actual_name = name\n",
    "                    if name == \"LFHF\" and \"LF/HF\" in hrv_cols:\n",
    "                        actual_name = \"LF/HF\"\n",
    "                    elif name == \"LF/HF\" and \"LFHF\" in hrv_cols:\n",
    "                         actual_name = \"LFHF\"\n",
    "\n",
    "                    if actual_name in hrv_cols and not np.isnan(hrv[actual_name].values[0]):\n",
    "                         ecg_feats.append(hrv[actual_name].values[0])\n",
    "                    else:\n",
    "                         ecg_feats.append(0.0) # Use 0.0 for missing/NaN features\n",
    "\n",
    "                 # Ensure we always have 10 features\n",
    "                if len(ecg_feats) < 10:\n",
    "                    ecg_feats.extend([0.0] * (10 - len(ecg_feats)))\n",
    "                elif len(ecg_feats) > 10:\n",
    "                     ecg_feats = ecg_feats[:10] # Truncate if more than 10 somehow\n",
    "\n",
    "            except Exception as e:\n",
    "                # Fallback: Compute basic statistics if advanced features fail.\n",
    "                # print(f\"Warning: ECG feature extraction failed for a window: {e}\") # Optional warning\n",
    "                basic_stats = [\n",
    "                     np.mean(ecg_signal) if len(ecg_signal) > 0 else 0.0,\n",
    "                     np.std(ecg_signal) if len(ecg_signal) > 1 else 0.0,\n",
    "                     np.min(ecg_signal) if len(ecg_signal) > 0 else 0.0,\n",
    "                     np.max(ecg_signal) if len(ecg_signal) > 0 else 0.0,\n",
    "                     np.median(ecg_signal) if len(ecg_signal) > 0 else 0.0\n",
    "                 ]\n",
    "                # Pad to reach length 10.\n",
    "                ecg_feats = basic_stats + [0.0] * (10 - len(basic_stats))\n",
    "        feat_list.append(np.array(ecg_feats))\n",
    "\n",
    "    # --- GSR Features (Consider nk.eda_phasic features: SCR_Peaks_Amplitude_Mean, etc.) ---\n",
    "    if 'gsr' in signals_dict:\n",
    "        gsr_signal = np.array(signals_dict['gsr']).flatten()\n",
    "        if len(gsr_signal) > 2: # Need at least 3 points for stats like skew/kurtosis\n",
    "            try:\n",
    "                # Use nk.eda_process for more robust feature extraction\n",
    "                eda_signals, info = nk.eda_process(gsr_signal, sampling_rate=fs)\n",
    "                num_scr_peaks = len(info[\"SCR_Peaks\"])\n",
    "                mean_scr_amp = np.mean(eda_signals[\"SCR_Amplitude\"]) if len(info[\"SCR_Peaks\"]) > 0 else 0.0\n",
    "                mean_scl = np.mean(eda_signals[\"EDA_Tonic\"])\n",
    "                std_scl = np.std(eda_signals[\"EDA_Tonic\"])\n",
    "\n",
    "                # Replace basic stats with these more informative ones + some basic stats\n",
    "                gsr_feats = [\n",
    "                     mean_scl, # Mean Tonic component\n",
    "                     std_scl, # Std Dev Tonic\n",
    "                     num_scr_peaks, # Number of SCR peaks in window\n",
    "                     mean_scr_amp, # Mean SCR Amplitude\n",
    "                     kurtosis(gsr_signal),\n",
    "                     skew(gsr_signal),\n",
    "                     np.std(gsr_signal) # Keep overall std dev\n",
    "                 ]\n",
    "            except Exception as e:\n",
    "                 # Fallback to basic stats if eda_process fails\n",
    "                 # print(f\"Warning: GSR feature extraction failed: {e}\") # Optional\n",
    "                 num_scr_peaks = 0.0 # Default if failed\n",
    "                 gsr_feats = [\n",
    "                     np.mean(gsr_signal), np.std(gsr_signal),\n",
    "                     np.min(gsr_signal), np.max(gsr_signal),\n",
    "                     kurtosis(gsr_signal) if len(gsr_signal)>3 else 0.0,\n",
    "                     skew(gsr_signal) if len(gsr_signal)>3 else 0.0,\n",
    "                     num_scr_peaks\n",
    "                 ]\n",
    "        else:\n",
    "             gsr_feats = [0.0] * 7 # Default for too short signal\n",
    "        feat_list.append(np.array(gsr_feats))\n",
    "\n",
    "    # --- EEG Features (Consider specific band powers: Alpha, Beta, Gamma, Theta, Delta & Ratios) ---\n",
    "    if 'eeg' in signals_dict:\n",
    "        eeg_data = np.array(signals_dict['eeg'])\n",
    "        if eeg_data.ndim == 1:\n",
    "            eeg_data = eeg_data[None, :]  # Ensure 2D shape [channels, time]\n",
    "\n",
    "        all_channels_feats = []\n",
    "        for ch in range(eeg_data.shape[0]):\n",
    "            channel_signal = eeg_data[ch, :]\n",
    "            # Need enough data points for reliable PSD calculation with Welch\n",
    "            min_len_welch = 256 # Or adjust nperseg based on window size\n",
    "            if len(channel_signal) < min_len_welch:\n",
    "                 ch_feats = [0.0] * 8 # Default value for all EEG features if too short\n",
    "            else:\n",
    "                try:\n",
    "                    # Hjorth parameters\n",
    "                    activity = np.var(channel_signal)\n",
    "                    diff_signal = np.diff(channel_signal)\n",
    "                    mobility = np.sqrt(np.var(diff_signal) / (activity + 1e-8))\n",
    "                    diff_diff_signal = np.diff(diff_signal)\n",
    "                    complexity = np.sqrt(np.var(diff_diff_signal) / (np.var(diff_signal) + 1e-8)) / (mobility + 1e-8)\n",
    "\n",
    "                    # Band power using Welch\n",
    "                    # Adjust nperseg based on window length, ensure it's not longer than signal\n",
    "                    nperseg = min(min_len_welch, len(channel_signal))\n",
    "                    freqs, psd = welch(channel_signal, fs=fs, nperseg=nperseg)\n",
    "\n",
    "                    def bandpower(f, pxx, fmin, fmax):\n",
    "                        idx = np.logical_and(f >= fmin, f <= fmax)\n",
    "                        if not np.any(idx): return 0.0 # Handle case where band is outside freq range\n",
    "                         # Use trapezoid integration\n",
    "                        return np.trapz(pxx[idx], x=f[idx])\n",
    "\n",
    "                    # Define frequency bands\n",
    "                    delta = bandpower(freqs, psd, 1, 4)\n",
    "                    theta = bandpower(freqs, psd, 4, 8)\n",
    "                    alpha = bandpower(freqs, psd, 8, 13) # Adjusted alpha range\n",
    "                    beta  = bandpower(freqs, psd, 13, 30) # Adjusted beta range\n",
    "                    gamma = bandpower(freqs, psd, 30, 45) # Limited gamma to match filtering\n",
    "\n",
    "                    # Combine Hjorth and band powers\n",
    "                    ch_feats = [activity, mobility, complexity, delta, theta, alpha, beta, gamma]\n",
    "\n",
    "                except Exception as e:\n",
    "                    # print(f\"Warning: EEG feature extraction failed for channel {ch}: {e}\") # Optional\n",
    "                    ch_feats = [0.0] * 8 # Default if any error occurs\n",
    "\n",
    "            all_channels_feats.append(ch_feats)\n",
    "\n",
    "        # Average features across channels (if multiple EEG channels were processed)\n",
    "        # If only one channel, this just returns that channel's features\n",
    "        eeg_feats = np.mean(all_channels_feats, axis=0)\n",
    "        feat_list.append(eeg_feats)\n",
    "\n",
    "    # ---------- Combine all features ----------\n",
    "    if len(feat_list) == 0:\n",
    "        # Return array of zeros with expected total feature dimension\n",
    "        # (10 ECG + 7 GSR + 8 EEG = 25 features)\n",
    "        return np.zeros(25)\n",
    "    # Concatenate features from all modalities processed\n",
    "    combined_features = np.concatenate(feat_list)\n",
    "\n",
    "    # Ensure consistent length (e.g., if one modality was missing)\n",
    "    expected_len = 25 # Update this if you change the number of features per modality\n",
    "    if len(combined_features) < expected_len:\n",
    "        combined_features = np.pad(combined_features, (0, expected_len - len(combined_features)), 'constant')\n",
    "    elif len(combined_features) > expected_len:\n",
    "        combined_features = combined_features[:expected_len] # Should not happen with careful implementation\n",
    "\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "# --- NEW FUNCTION: Build Windowed Dataset ---\n",
    "def build_windowed_dataset(joined_data, labels_array):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    n_trials = joined_data.shape[1]\n",
    "\n",
    "    print(f\"Processing {n_trials} trials for windowing...\")\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # 1. Get preprocessed signal for the *entire* trial\n",
    "        #    Set target_length=None as windowing handles length implicitly\n",
    "        trial_signal_processed = process_trial_signal(joined_data[0, i], target_length=None, fs=512)\n",
    "\n",
    "        # --- ADD DEBUG PRINTS ---\n",
    "        print(f\"  Trial {i}: Processed signal shape = {trial_signal_processed.shape}\")\n",
    "        if trial_signal_processed.shape[1] > 0: # Avoid errors if shape is (n, 0)\n",
    "            print(f\"            Signal length = {trial_signal_processed.shape[1]} samples. Required >= {WINDOW_SAMPLES}\")\n",
    "        # --- END DEBUG PRINTS ---\n",
    "\n",
    "        if trial_signal_processed.size == 0 or trial_signal_processed.shape[1] < WINDOW_SAMPLES:\n",
    "            print(f\"  Trial {i}: Skipping - Signal empty or too short.\") # Simplified message\n",
    "            continue # Skip trial if too short for even one window\n",
    "\n",
    "        # 2. Get the corresponding label for this trial\n",
    "        lbl = np.array(labels_array[0, i]).squeeze()\n",
    "        if lbl.size < 3:\n",
    "            # print(f\"Warning: Trial {i} does not have enough label data. Skipping trial.\")\n",
    "            continue\n",
    "        selected_label = lbl[1:3]  # use only valence and arousal\n",
    "        discrete_label = discretize_label(selected_label)\n",
    "        if discrete_label == \"Unknown\":\n",
    "             # print(f\"Warning: Trial {i} has unknown label. Skipping trial.\")\n",
    "             continue\n",
    "\n",
    "        # 3. Iterate through windows of the trial signal\n",
    "        num_samples_trial = trial_signal_processed.shape[1]\n",
    "        for start in range(0, num_samples_trial - WINDOW_SAMPLES + 1, STEP_SAMPLES):\n",
    "            end = start + WINDOW_SAMPLES\n",
    "            signal_window = trial_signal_processed[:, start:end] # Shape [channels, WINDOW_SAMPLES]\n",
    "\n",
    "            # 4. Extract features for this window\n",
    "            signals_dict = split_into_modalities(signal_window)\n",
    "            if not signals_dict: # Check if split failed\n",
    "                 continue\n",
    "            window_features = extract_features(signals_dict, fs=FS)\n",
    "\n",
    "            # 5. Append features and the trial's label\n",
    "            X_list.append(window_features)\n",
    "            y_list.append(discrete_label)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        print(\"Warning: No windows generated from the data.\")\n",
    "        return None, None\n",
    "\n",
    "    X_array = np.vstack(X_list)\n",
    "    y_array = np.array(y_list)\n",
    "    print(f\"Generated {X_array.shape[0]} windows from {n_trials} trials.\")\n",
    "    return X_array, y_array\n",
    "\n",
    "\n",
    "# --- NEW FUNCTION: Load all patient data using windowing ---\n",
    "def load_all_patients_windowed_data(num_patients=40):\n",
    "    X_list_all = []\n",
    "    y_list_all = []\n",
    "    for patient in range(1, num_patients + 1):\n",
    "        print(f\"\\nLoading and windowing patient {patient}...\")\n",
    "        try:\n",
    "            data = load_patient_preprocessed_data(patient)\n",
    "            joined_data = data['joined_data']\n",
    "            labels_array = data['labels_ext_annotation']\n",
    "\n",
    "            # Use the new windowed function\n",
    "            X_patient_windows, y_patient_windows = build_windowed_dataset(joined_data, labels_array)\n",
    "\n",
    "            if X_patient_windows is not None and y_patient_windows is not None:\n",
    "                X_list_all.append(X_patient_windows)\n",
    "                y_list_all.append(y_patient_windows)\n",
    "            else:\n",
    "                 print(f\"No windows generated for patient {patient}.\")\n",
    "            # Clean up memory\n",
    "            del data, joined_data, labels_array, X_patient_windows, y_patient_windows\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing patient {patient}: {e}\")\n",
    "\n",
    "\n",
    "    if not X_list_all: # Check if the list is empty\n",
    "        raise ValueError(\"No windowed data loaded for any patient.\")\n",
    "\n",
    "    X_all_windows = np.vstack(X_list_all)\n",
    "    y_all_windows = np.concatenate(y_list_all)\n",
    "\n",
    "    print(f\"\\nTotal windows loaded from all patients: {X_all_windows.shape[0]}\")\n",
    "    return X_all_windows, y_all_windows\n",
    "\n",
    "# --- Remove old loading functions not needed now ---\n",
    "# def build_patient_data(joined_data, label_array): ... # Removed\n",
    "# def load_all_patients_data(num_patients=40): ... # Removed\n",
    "# def load_all_patients_raw_signal(num_patients=40, target_length=None): ... # Removed\n",
    "# def pad_trials(trials, pad_mode='constant', constant_values=0): ... # Removed (padding within windowing/feature extraction if needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the NEW windowed data ---\n",
    "print(\"Loading windowed data for all patients...\")\n",
    "# Set num_patients lower (e.g., 5) for initial testing to speed things up\n",
    "X_windows, y_windows_desc = load_all_patients_windowed_data(num_patients=40) # ADJUST number of patients!\n",
    "\n",
    "# --- Prepare labels for windowed data ---\n",
    "print(\"\\nPreparing labels...\")\n",
    "unique_labels_w = np.unique(y_windows_desc)\n",
    "label_to_int_w = {label: idx for idx, label in enumerate(unique_labels_w)}\n",
    "y_int_windows = np.array([label_to_int_w[label] for label in y_windows_desc])\n",
    "# y_cat_windows = to_categorical(y_int_windows) # Use if needed for specific models like Keras NN\n",
    "\n",
    "print(f\"Window data shape: {X_windows.shape}\")\n",
    "print(f\"Window labels shape: {y_int_windows.shape}\")\n",
    "print(f\"Class distribution in windowed data: {np.bincount(y_int_windows)}\")\n",
    "\n",
    "# --- Split windowed data ---\n",
    "# Stratify based on the window labels\n",
    "X_train_w, X_test_w, y_train_labels_w, y_test_labels_w, y_int_train_w, y_int_test_w = train_test_split(\n",
    "    X_windows, y_windows_desc, y_int_windows,\n",
    "    test_size=0.2, random_state=42, stratify=y_int_windows\n",
    ")\n",
    "\n",
    "print(f\"Train window data shape: {X_train_w.shape}\")\n",
    "print(f\"Test window data shape: {X_test_w.shape}\")\n",
    "print(f\"Class distribution in train windows: {np.bincount(y_int_train_w)}\")\n",
    "print(f\"Class distribution in test windows: {np.bincount(y_int_test_w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Statistical Feature Extraction + ML classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing windowed data...\n",
      "Imputation complete.\n",
      "Scaling complete.\n",
      "Final training data shape: (45554, 25)\n",
      "Final test data shape: (11389, 25)\n",
      "\n",
      "Evaluating classifiers with 5-fold Stratified cross-validation (on WINDOWED train data):\n",
      "SVM: Mean Balanced Accuracy = 0.462 (+/- 0.009)\n",
      "KNN: Mean Balanced Accuracy = 0.321 (+/- 0.004)\n",
      "Random Forest: Mean Balanced Accuracy = 0.366 (+/- 0.005)\n",
      "Gradient Boosting: Mean Balanced Accuracy = 0.324 (+/- 0.004)\n",
      "Extra Trees: Mean Balanced Accuracy = 0.492 (+/- 0.014)\n",
      "\n",
      "--- Starting Hyperparameter Tuning ---\n",
      "\n",
      "Models selected for tuning based on initial CV (Windowed Data, Balanced Acc >= 0.343): ['Extra Trees', 'SVM', 'Random Forest']\n",
      "\n",
      "Tuning Extra Trees...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for Extra Trees: {'class_weight': 'balanced', 'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 400}\n",
      "Best CV balanced accuracy for Extra Trees (on windowed data): 0.4853\n",
      "\n",
      "Tuning SVM...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "GridSearchCV for SVM failed: [WinError 1450] Insufficient system resources exist to complete the requested service\n",
      "\n",
      "Tuning Random Forest...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for Random Forest: {'class_weight': 'balanced', 'max_depth': 40, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best CV balanced accuracy for Random Forest (on windowed data): 0.3888\n",
      "\n",
      "--- Best Overall Model based on Tuning CV Score (Windowed Data): Extra Trees (0.4853) ---\n",
      "\n",
      "Evaluating best model (Extra Trees) on the test windows...\n",
      "\n",
      "Final Extra Trees Accuracy on Test Windows: 0.6627\n",
      "Final Extra Trees Balanced Accuracy on Test Windows: 0.4822\n",
      "\n",
      "Confusion Matrix (Test Windows):\n",
      "[[ 821  142   40  766]\n",
      " [ 161  658   27  839]\n",
      " [  18   18   52   86]\n",
      " [ 854  785  105 6017]]\n",
      "\n",
      "Classification Report (Test Windows):\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "High valence, High arousal      0.443     0.464     0.453      1769\n",
      " High valence, Low arousal      0.410     0.391     0.400      1685\n",
      " Low valence, High arousal      0.232     0.299     0.261       174\n",
      "  Low valence, Low arousal      0.781     0.775     0.778      7761\n",
      "\n",
      "                  accuracy                          0.663     11389\n",
      "                 macro avg      0.467     0.482     0.473     11389\n",
      "              weighted avg      0.665     0.663     0.664     11389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "import scipy.io\n",
    "import scipy.signal as sgl\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold # Consider using feature selection later\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, balanced_accuracy_score # Make sure balanced is imported\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "# from sklearn.decomposition import PCA # Not using PCA now\n",
    "# from imblearn.over_sampling import SMOTE # Not using SMOTE now\n",
    "import xgboost as xgb # Using the updated XGBoost\n",
    "# import lightgbm as lgb # Still removed as requested previously\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "# --- Preprocessing for Windowed Features ---\n",
    "# Apply Imputation and Scaling to the windowed data\n",
    "# These variables (X_train_w, X_test_w, y_int_train_w, y_int_test_w etc.)\n",
    "# should be available from the previous cell execution.\n",
    "print(\"Preprocessing windowed data...\")\n",
    "\n",
    "# Imputation - Fit on training windows ONLY\n",
    "# Handle case where X_train_w might be empty if loading failed\n",
    "if 'X_train_w' in locals() and X_train_w.size > 0:\n",
    "    imputer_w = SimpleImputer(strategy='mean').fit(X_train_w)\n",
    "    X_train_w_imputed = imputer_w.transform(X_train_w)\n",
    "    X_test_w_imputed = imputer_w.transform(X_test_w) # Use the same imputer for test set\n",
    "    print(\"Imputation complete.\")\n",
    "\n",
    "    # Check for NaNs/Infs after imputation (important sanity check)\n",
    "    if np.any(np.isnan(X_train_w_imputed)) or np.any(np.isinf(X_train_w_imputed)):\n",
    "        print(\"Warning: NaNs or Infs found in training data AFTER imputation! Replacing with 0.\")\n",
    "        X_train_w_imputed = np.nan_to_num(X_train_w_imputed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if np.any(np.isnan(X_test_w_imputed)) or np.any(np.isinf(X_test_w_imputed)):\n",
    "        print(\"Warning: NaNs or Infs found in test data AFTER imputation! Replacing with 0.\")\n",
    "        X_test_w_imputed = np.nan_to_num(X_test_w_imputed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Global normalization - Fit on training windows ONLY\n",
    "    scaler_w = StandardScaler().fit(X_train_w_imputed)\n",
    "    X_train_final = scaler_w.transform(X_train_w_imputed) # Use this for training/CV\n",
    "    X_test_final = scaler_w.transform(X_test_w_imputed)   # Use this for final testing\n",
    "    y_train_final = y_int_train_w                         # Integer labels for training\n",
    "    y_test_final = y_int_test_w                           # Integer labels for testing\n",
    "    print(\"Scaling complete.\")\n",
    "    print(f\"Final training data shape: {X_train_final.shape}\")\n",
    "    print(f\"Final test data shape: {X_test_final.shape}\")\n",
    "\n",
    "    # --- Define Cross-Validation Strategy (Stratified) ---\n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # --- Define Classifiers (Using class_weight='balanced') ---\n",
    "    classifiers = {\n",
    "        # \"Logistic Regression\": LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42, solver='liblinear'),\n",
    "        \"SVM\": SVC(probability=True, class_weight='balanced', random_state=42, cache_size=700), # Added cache_size\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5), # Consider larger n_neighbors based on CV\n",
    "        # \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=20), # Limit depth\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1, max_depth=20), # Limit depth initially\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5), # Limit depth\n",
    "        \"Extra Trees\": ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1, max_depth=20), # Limit depth initially\n",
    "        # \"XGBoost\": xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', use_label_encoder=False, random_state=42, n_jobs=-1), # Keep commented if still issues\n",
    "    }\n",
    "\n",
    "    # --- Optional: Initial Cross-Validation (on WINDOWED imbalanced train data) ---\n",
    "    results_cv = {}\n",
    "    print(\"\\nEvaluating classifiers with 5-fold Stratified cross-validation (on WINDOWED train data):\")\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        try:\n",
    "            # Use the windowed X_train_final, y_train_final and stratified CV\n",
    "            # Increase CV timeout if needed for larger data\n",
    "            cv_scores = cross_val_score(clf, X_train_final, y_train_final, cv=cv_strategy, scoring='balanced_accuracy', n_jobs=-1, error_score='raise') # Add error_score='raise'\n",
    "            results_cv[clf_name] = np.mean(cv_scores)\n",
    "            print(f\"{clf_name}: Mean Balanced Accuracy = {np.mean(cv_scores):.3f} (+/- {np.std(cv_scores):.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"{clf_name} encountered an error during cross-validation: {e}\")\n",
    "\n",
    "\n",
    "    # --- Hyperparameter Tuning using GridSearchCV ---\n",
    "    print(\"\\n--- Starting Hyperparameter Tuning ---\")\n",
    "    # Adjusted grids slightly, focusing on parameters likely affected by more data\n",
    "    param_grids = {\n",
    "        \"Extra Trees\": {\n",
    "            'n_estimators': [200, 400], # Increased estimators\n",
    "            'max_depth': [30, 40, None], # Deeper trees might be possible\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 3],\n",
    "            'class_weight': ['balanced']\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            'n_estimators': [200, 400],\n",
    "            'max_depth': [30, 40, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 3],\n",
    "            'class_weight': ['balanced']\n",
    "        },\n",
    "        \"SVM\": {\n",
    "            'C': [1, 10, 50], # Keep C range reasonable\n",
    "            'gamma': ['scale', 0.01], # Scale is often good, added 0.01\n",
    "            'kernel': ['rbf'],\n",
    "            'class_weight': ['balanced']\n",
    "         },\n",
    "         \"KNN\": {\n",
    "            'n_neighbors': [9, 15, 21, 29] # Explore more neighbors\n",
    "         }\n",
    "        # Add XGBoost back here if it's working\n",
    "    }\n",
    "\n",
    "    best_estimators = {}\n",
    "    best_cv_scores = {}\n",
    "\n",
    "    # Select models for tuning based on the NEW CV results on windowed data\n",
    "    if results_cv:\n",
    "        sorted_cv = sorted(results_cv.items(), key=lambda item: item[1], reverse=True)\n",
    "        # Adjust threshold based on observed CV results\n",
    "        tune_threshold = max(0.30, np.mean(list(results_cv.values())) - 0.05) if results_cv else 0.30\n",
    "        models_to_tune = [name for name, score in sorted_cv if name in param_grids and score >= tune_threshold]\n",
    "        print(f\"\\nModels selected for tuning based on initial CV (Windowed Data, Balanced Acc >= {tune_threshold:.3f}): {models_to_tune}\")\n",
    "    else:\n",
    "        print(\"\\nNo initial CV results available, attempting to tune all models defined in param_grids.\")\n",
    "        models_to_tune = [name for name in param_grids if name in classifiers]\n",
    "\n",
    "\n",
    "    for name in models_to_tune:\n",
    "        if name in classifiers and name in param_grids:\n",
    "            print(f\"\\nTuning {name}...\")\n",
    "            clf = classifiers[name]\n",
    "            # Use windowed data and stratified CV for tuning\n",
    "            # Reduce verbosity if output is too long\n",
    "            grid_search = GridSearchCV(estimator=clf, param_grid=param_grids[name],\n",
    "                                       cv=cv_strategy, # Use stratified CV strategy\n",
    "                                       n_jobs=-1, verbose=1, scoring='balanced_accuracy')\n",
    "            try:\n",
    "                # Fit on windowed training data\n",
    "                grid_search.fit(X_train_final, y_train_final)\n",
    "                best_estimators[name] = grid_search.best_estimator_\n",
    "                best_cv_scores[name] = grid_search.best_score_\n",
    "                print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "                print(f\"Best CV balanced accuracy for {name} (on windowed data): {grid_search.best_score_:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"GridSearchCV for {name} failed: {e}\")\n",
    "        else:\n",
    "             print(f\"Skipping tuning for {name} (not in classifiers or param_grids defined for tuning)\")\n",
    "\n",
    "\n",
    "    # --- Select the overall best model based on tuning CV score ---\n",
    "    if best_estimators: # Check if any tuning was successful\n",
    "        valid_scores = {name: score for name, score in best_cv_scores.items() if score is not None and not np.isnan(score)}\n",
    "        if valid_scores:\n",
    "            best_model_name = max(valid_scores, key=valid_scores.get)\n",
    "            best_overall_model = best_estimators[best_model_name]\n",
    "            print(f\"\\n--- Best Overall Model based on Tuning CV Score (Windowed Data): {best_model_name} ({valid_scores[best_model_name]:.4f}) ---\")\n",
    "\n",
    "            # --- Final Evaluation on Test Set using the BEST Tuned Model (Windowed Data) ---\n",
    "            print(f\"\\nEvaluating best model ({best_model_name}) on the test windows...\")\n",
    "            # Predict on the scaled test set windows (X_test_final)\n",
    "            predictions = best_overall_model.predict(X_test_final)\n",
    "            final_acc = accuracy_score(y_test_final, predictions) # Evaluate against test window labels\n",
    "            final_balanced_acc = balanced_accuracy_score(y_test_final, predictions)\n",
    "\n",
    "            print(f\"\\nFinal {best_model_name} Accuracy on Test Windows: {final_acc:.4f}\")\n",
    "            print(f\"Final {best_model_name} Balanced Accuracy on Test Windows: {final_balanced_acc:.4f}\") # Key metric for window performance\n",
    "            print(\"\\nConfusion Matrix (Test Windows):\")\n",
    "            print(confusion_matrix(y_test_final, predictions))\n",
    "            print(\"\\nClassification Report (Test Windows):\")\n",
    "            # Ensure label_to_int_w exists from the previous cell\n",
    "            if 'label_to_int_w' in locals():\n",
    "                 target_names_w = [label for label, idx in sorted(label_to_int_w.items(), key=lambda item: item[1])]\n",
    "                 print(classification_report(y_test_final, predictions, target_names=target_names_w, digits=3))\n",
    "            else:\n",
    "                 print(\"Warning: label_to_int_w not found. Cannot print target names in report.\")\n",
    "                 print(classification_report(y_test_final, predictions, digits=3))\n",
    "\n",
    "            # Note: This evaluation is per-window. To get per-trial accuracy, you would need\n",
    "            # to group predictions by trial and use a voting/aggregation strategy.\n",
    "        else:\n",
    "            print(\"\\nNo models completed tuning successfully with a valid score (Windowed Data). Cannot evaluate final performance.\")\n",
    "    else:\n",
    "        print(\"\\nNo models were successfully tuned or tuning list was empty (Windowed Data). Cannot evaluate final performance.\")\n",
    "\n",
    "else:\n",
    "     print(\"\\nError: Windowed training data ('X_train_w') not found or is empty. Cannot proceed with ML pipeline.\")\n",
    "     # Add any necessary cleanup or exit logic here if required"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
