{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a424fa5",
   "metadata": {
    "id": "8-o5Wfg9YtAB",
    "papermill": {
     "duration": 0.005437,
     "end_time": "2024-12-14T02:18:34.734715",
     "exception": false,
     "start_time": "2024-12-14T02:18:34.729278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load sub\n",
    "\n",
    "The videos are in the order of Experiment_id, so not in the order of presentation. This means the first video is the same for each participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eaaf2f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:18:34.746205Z",
     "iopub.status.busy": "2024-12-14T02:18:34.745887Z",
     "iopub.status.idle": "2024-12-14T02:18:52.790414Z",
     "shell.execute_reply": "2024-12-14T02:18:52.789497Z"
    },
    "id": "vXChgENjXQLH",
    "papermill": {
     "duration": 18.053235,
     "end_time": "2024-12-14T02:18:52.792610",
     "exception": false,
     "start_time": "2024-12-14T02:18:34.739375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrmr-selection\n",
      "  Downloading mrmr_selection-0.2.8-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting category-encoders (from mrmr-selection)\n",
      "  Downloading category_encoders-2.8.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mrmr-selection) (3.1.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mrmr-selection) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mrmr-selection) (1.4.2)\n",
      "Requirement already satisfied: pandas>=1.0.3 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mrmr-selection) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mrmr-selection) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mrmr-selection) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mrmr-selection) (1.15.1)\n",
      "Collecting polars>=0.12.5 (from mrmr-selection)\n",
      "  Downloading polars-1.21.0-cp39-abi3-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.0.3->mrmr-selection) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.0.3->mrmr-selection) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.0.3->mrmr-selection) (2024.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category-encoders->mrmr-selection) (1.0.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category-encoders->mrmr-selection) (0.14.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->mrmr-selection) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->mrmr-selection) (3.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->mrmr-selection) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.3->mrmr-selection) (1.17.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\ferri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from statsmodels>=0.9.0->category-encoders->mrmr-selection) (24.2)\n",
      "Downloading mrmr_selection-0.2.8-py3-none-any.whl (15 kB)\n",
      "Downloading polars-1.21.0-cp39-abi3-win_amd64.whl (31.7 MB)\n",
      "   ---------------------------------------- 0.0/31.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/31.7 MB 8.5 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.9/31.7 MB 8.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 4.7/31.7 MB 8.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 6.6/31.7 MB 8.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 8.4/31.7 MB 8.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 10.2/31.7 MB 8.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 12.1/31.7 MB 8.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 13.9/31.7 MB 8.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 15.5/31.7 MB 8.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 17.0/31.7 MB 8.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 18.9/31.7 MB 8.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 20.7/31.7 MB 8.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 22.3/31.7 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 24.1/31.7 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 26.2/31.7 MB 8.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 27.8/31.7 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 29.6/31.7 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  31.5/31.7 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 31.7/31.7 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading category_encoders-2.8.0-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: polars, category-encoders, mrmr-selection\n",
      "Successfully installed category-encoders-2.8.0 mrmr-selection-0.2.8 polars-1.21.0\n",
      "Collecting ordpy\n",
      "  Downloading ordpy-1.1.5-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading ordpy-1.1.5-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: ordpy\n",
      "Successfully installed ordpy-1.1.5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import gc  # Garbage collector\n",
    "\n",
    "!pip install mrmr-selection\n",
    "!pip install ordpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b633346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:18:52.804599Z",
     "iopub.status.busy": "2024-12-14T02:18:52.804335Z",
     "iopub.status.idle": "2024-12-14T02:19:19.650699Z",
     "shell.execute_reply": "2024-12-14T02:19:19.649733Z"
    },
    "papermill": {
     "duration": 26.854463,
     "end_time": "2024-12-14T02:19:19.652606",
     "exception": false,
     "start_time": "2024-12-14T02:18:52.798143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 40, 8064)\n",
      "(1280, 4)\n"
     ]
    }
   ],
   "source": [
    "participants = 32\n",
    "subjects = {'data': [], 'labels': []}\n",
    "prefix_path = 'C:\\\\Users\\\\ferri\\\\Downloads\\\\PoliTO\\\\Tesi\\\\DSs\\\\Stress\\\\deap-dataset\\\\'\n",
    "\n",
    "# Read and collect data\n",
    "for i in range(1, participants + 1):\n",
    "    file_name = prefix_path + f\"data_preprocessed_python\\\\s{'0' if i < 10 else ''}{i}.dat\"\n",
    "    with open(file_name, 'rb') as file:\n",
    "        subject = pickle.load(file, encoding='latin1')\n",
    "        for key in subjects:\n",
    "            subjects[key].append(subject[key])\n",
    "\n",
    "        del subject\n",
    "\n",
    "# Merge data and reshape\n",
    "for key in subjects:\n",
    "    subjects[key] = np.concatenate(subjects[key], axis=0)\n",
    "\n",
    "print(subjects['data'].shape)\n",
    "print(subjects['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7e3345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:19.664575Z",
     "iopub.status.busy": "2024-12-14T02:19:19.664028Z",
     "iopub.status.idle": "2024-12-14T02:19:19.670727Z",
     "shell.execute_reply": "2024-12-14T02:19:19.669937Z"
    },
    "papermill": {
     "duration": 0.014383,
     "end_time": "2024-12-14T02:19:19.672409",
     "exception": false,
     "start_time": "2024-12-14T02:19:19.658026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0}\n",
      "{0.0, 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Use 2 classifiers valence and arousal\n",
    "subjects['labels'] = subjects['labels'][:, :2]\n",
    "\n",
    "# Use threshold 5 to assign binary label to each classifier\n",
    "subjects['labels'][:, 0] = (subjects['labels'][:, 0] >= 5).astype(int)\n",
    "subjects['labels'][:, 1] = (subjects['labels'][:, 1] >= 5).astype(int)\n",
    "\n",
    "print(set(subjects['labels'][:, 0]))\n",
    "print(set(subjects['labels'][:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96081c1",
   "metadata": {
    "papermill": {
     "duration": 0.005,
     "end_time": "2024-12-14T02:19:19.682532",
     "exception": false,
     "start_time": "2024-12-14T02:19:19.677532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split Train Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c47127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:19.693561Z",
     "iopub.status.busy": "2024-12-14T02:19:19.693342Z",
     "iopub.status.idle": "2024-12-14T02:19:21.756931Z",
     "shell.execute_reply": "2024-12-14T02:19:21.756102Z"
    },
    "papermill": {
     "duration": 2.070958,
     "end_time": "2024-12-14T02:19:21.758539",
     "exception": false,
     "start_time": "2024-12-14T02:19:19.687581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 40, 8064) (256, 40, 8064) (1024, 2) (256, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    subjects['data'], subjects['labels'], test_size=test_size, random_state=random_state\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "del subjects\n",
    "gc.collect()  # Free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f95fd",
   "metadata": {
    "papermill": {
     "duration": 0.005846,
     "end_time": "2024-12-14T02:19:21.769933",
     "exception": false,
     "start_time": "2024-12-14T02:19:21.764087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665ba0c",
   "metadata": {
    "papermill": {
     "duration": 0.00644,
     "end_time": "2024-12-14T02:19:21.784501",
     "exception": false,
     "start_time": "2024-12-14T02:19:21.778061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Wavelet transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c385adc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:21.796792Z",
     "iopub.status.busy": "2024-12-14T02:19:21.795935Z",
     "iopub.status.idle": "2024-12-14T02:19:21.873590Z",
     "shell.execute_reply": "2024-12-14T02:19:21.872726Z"
    },
    "papermill": {
     "duration": 0.0855,
     "end_time": "2024-12-14T02:19:21.875322",
     "exception": false,
     "start_time": "2024-12-14T02:19:21.789822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wavelet transform\n",
    "import pywt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sampling frequency:\n",
    "fs = 128\n",
    "\n",
    "# Select scales and wavelets\n",
    "scales = np.arange(1, 129)  # Example: 129 scale values\n",
    "wavelet = 'morl'\n",
    "\n",
    "def CWT_for_sample(data):\n",
    "    cwt_output = []\n",
    "    # Calculate CWT for each sample and each channel\n",
    "    for i in tqdm(range(data.shape[0])):  # 1024 samples\n",
    "        cwt_channels = []\n",
    "        for j in range(data.shape[1]):  # 40 channels\n",
    "            coeffs, freqs = pywt.cwt(data[i, j], scales, wavelet) # coeffs are of the form (len(scales), 8064)\n",
    "            energy_scales = np.sum(np.abs(coeffs)**2, axis=1)  # (len(scales),)\n",
    "            cwt_channels.append(energy_scales)\n",
    "        cwt_output.append(cwt_channels)\n",
    "    \n",
    "    # Convert output to numpy array\n",
    "    cwt_output = np.array(cwt_output)  # Size: (1024, 40, len(scales))\n",
    "\n",
    "    return cwt_output\n",
    "    \n",
    "# cwt_train = CWT_for_sample(X_train) # Size: (1024, 40, len(scales))\n",
    "# cwt_test = CWT_for_sample(X_test) # Size: (256, 40, len(scales))\n",
    "\n",
    "# print(cwt_train.shape, cwt_test.shape)\n",
    "\n",
    "# # Save file name\n",
    "# prefix_output_path = '/kaggle/working/'\n",
    "# filename = \"cwt_data.pkl\"\n",
    "\n",
    "# # Save data\n",
    "# with open(filename, 'wb') as file:\n",
    "#     pickle.dump({'train': cwt_train, 'test': cwt_test}, file)\n",
    "\n",
    "# print(f\"Data has been saved to file {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11c8f6d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:21.894280Z",
     "iopub.status.busy": "2024-12-14T02:19:21.893393Z",
     "iopub.status.idle": "2024-12-14T02:19:22.386639Z",
     "shell.execute_reply": "2024-12-14T02:19:22.385524Z"
    },
    "papermill": {
     "duration": 0.505542,
     "end_time": "2024-12-14T02:19:22.388501",
     "exception": false,
     "start_time": "2024-12-14T02:19:21.882959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size cwt_train: (1024, 40, 128)\n",
      "Size cwt_test: (256, 40, 128)\n"
     ]
    }
   ],
   "source": [
    "# Save file name\n",
    "prefix_path = 'C:\\\\Users\\\\ferri\\\\Downloads\\\\PoliTO\\\\Tesi\\\\DSs\\\\Stress\\\\deap-dataset\\\\'\n",
    "filename = \"cwt_data.pkl\"\n",
    "\n",
    "# Read data again\n",
    "with open(prefix_path + filename, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Data retrieval\n",
    "cwt_train_loaded = data['train']\n",
    "cwt_test_loaded = data['test']\n",
    "\n",
    "print(\"Size cwt_train:\", cwt_train_loaded.shape)\n",
    "print(\"Size cwt_test:\", cwt_test_loaded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ef55a",
   "metadata": {
    "papermill": {
     "duration": 0.00529,
     "end_time": "2024-12-14T02:19:22.399337",
     "exception": false,
     "start_time": "2024-12-14T02:19:22.394047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Nonlinear Feature Analyses: Permutation Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd705ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:22.411187Z",
     "iopub.status.busy": "2024-12-14T02:19:22.410904Z",
     "iopub.status.idle": "2024-12-14T02:19:22.418960Z",
     "shell.execute_reply": "2024-12-14T02:19:22.418178Z"
    },
    "papermill": {
     "duration": 0.015777,
     "end_time": "2024-12-14T02:19:22.420542",
     "exception": false,
     "start_time": "2024-12-14T02:19:22.404765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ordpy\n",
    "\n",
    "def split_array(arr, num_epochs):\n",
    "    \n",
    "    len_arr = len(arr)\n",
    "    len_epochs = round(len_arr / num_epochs)\n",
    "    \n",
    "    splits_arr = [arr[i * len_epochs: len_arr if i + 1 == num_epochs else (i + 1) * len_epochs] for i in range(num_epochs)]\n",
    "\n",
    "    return splits_arr\n",
    "\n",
    "def permutation_entropy_eeg(data, num_epochs=8):\n",
    "    B, C, T = data.shape\n",
    "    pe = np.zeros((B, C, num_epochs))\n",
    "    \n",
    "    for b in tqdm(range(B)):\n",
    "        for c in range(C):\n",
    "            splits_arr = split_array(data[b, c], num_epochs)\n",
    "            for i in range(num_epochs):\n",
    "                pe[b, c, i] = ordpy.permutation_entropy(splits_arr[i])\n",
    "\n",
    "    return pe\n",
    "\n",
    "# pe_train = permutation_entropy_eeg(X_train)#1024x40\n",
    "# pe_test = permutation_entropy_eeg(X_test)#256x40\n",
    "\n",
    "# # Save file name\n",
    "# prefix_output_path = '/kaggle/working/'\n",
    "# filename = \"pe_data.pkl\"\n",
    "\n",
    "# # Save data\n",
    "# with open(filename, 'wb') as file:\n",
    "#    pickle.dump({'train': pe_train, 'test': pe_test}, file)\n",
    "\n",
    "# print(f\"Data has been saved to file {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ff9e1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:22.432770Z",
     "iopub.status.busy": "2024-12-14T02:19:22.432140Z",
     "iopub.status.idle": "2024-12-14T02:19:22.481085Z",
     "shell.execute_reply": "2024-12-14T02:19:22.480130Z"
    },
    "papermill": {
     "duration": 0.056838,
     "end_time": "2024-12-14T02:19:22.482775",
     "exception": false,
     "start_time": "2024-12-14T02:19:22.425937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size pe_train: (1024, 40, 8)\n",
      "Size pe_test: (256, 40, 8)\n"
     ]
    }
   ],
   "source": [
    "# # Save file name\n",
    "prefix_path = 'C:\\\\Users\\\\ferri\\\\Downloads\\\\PoliTO\\\\Tesi\\\\DSs\\\\Stress\\\\deap-dataset\\\\'\n",
    "filename = \"pe_data.pkl\"\n",
    "\n",
    "# Read data again\n",
    "with open(prefix_path + filename, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Data retrieval\n",
    "pe_train_loaded = data['train']\n",
    "pe_test_loaded = data['test']\n",
    "\n",
    "print(\"Size pe_train:\", pe_train_loaded.shape)\n",
    "print(\"Size pe_test:\", pe_test_loaded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b50eb9",
   "metadata": {
    "papermill": {
     "duration": 0.005191,
     "end_time": "2024-12-14T02:19:22.493569",
     "exception": false,
     "start_time": "2024-12-14T02:19:22.488378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b5501a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:22.505170Z",
     "iopub.status.busy": "2024-12-14T02:19:22.504942Z",
     "iopub.status.idle": "2024-12-14T02:19:22.599980Z",
     "shell.execute_reply": "2024-12-14T02:19:22.599349Z"
    },
    "papermill": {
     "duration": 0.10266,
     "end_time": "2024-12-14T02:19:22.601614",
     "exception": false,
     "start_time": "2024-12-14T02:19:22.498954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca_data(data, k):\n",
    "    \n",
    "    # Reshape the data into (n_samples, n_features) to prepare for PCA\n",
    "    n_samples, n_channels, n_features = data.shape\n",
    "    data_reshaped = data.reshape(n_samples * n_channels, n_features)\n",
    "    \n",
    "    # Data Normalization\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_reshaped)\n",
    "    \n",
    "    # Apply PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=k)\n",
    "    data_pca = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Reshape back to (1024, 40, k)\n",
    "    data_pca_reshaped = data_pca.reshape(n_samples, n_channels, k)\n",
    "    \n",
    "    return data_pca_reshaped\n",
    "\n",
    "# pca_train = pca_data(X_train, k=128)\n",
    "# pca_test = pca_data(X_test, k=128)\n",
    "\n",
    "# # Save file name\n",
    "# prefix_output_path = '/kaggle/working/'\n",
    "# filename = \"pca_data.pkl\"\n",
    "\n",
    "# # Save data\n",
    "# with open(filename, 'wb') as file:\n",
    "#    pickle.dump({'train': pca_train, 'test': pca_test}, file)\n",
    "\n",
    "# print(f\"Data has been saved to file {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7829a243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:22.613557Z",
     "iopub.status.busy": "2024-12-14T02:19:22.613328Z",
     "iopub.status.idle": "2024-12-14T02:19:23.096634Z",
     "shell.execute_reply": "2024-12-14T02:19:23.095590Z"
    },
    "papermill": {
     "duration": 0.49133,
     "end_time": "2024-12-14T02:19:23.098554",
     "exception": false,
     "start_time": "2024-12-14T02:19:22.607224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size pca_train: (1024, 40, 128)\n",
      "Size pca_test: (256, 40, 128)\n"
     ]
    }
   ],
   "source": [
    "# Save file name\n",
    "prefix_path = 'C:\\\\Users\\\\ferri\\\\Downloads\\\\PoliTO\\\\Tesi\\\\DSs\\\\Stress\\\\deap-dataset\\\\'\n",
    "filename = \"pca_data.pkl\"\n",
    "\n",
    "# Read data again\n",
    "with open(prefix_path + filename, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Data retrieval\n",
    "pca_train_loaded = data['train']\n",
    "pca_test_loaded = data['test']\n",
    "\n",
    "print(\"Size pca_train:\", pca_train_loaded.shape)\n",
    "print(\"Size pca_test:\", pca_test_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3cd3a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:23.111006Z",
     "iopub.status.busy": "2024-12-14T02:19:23.110733Z",
     "iopub.status.idle": "2024-12-14T02:19:24.538505Z",
     "shell.execute_reply": "2024-12-14T02:19:24.537851Z"
    },
    "papermill": {
     "duration": 1.436435,
     "end_time": "2024-12-14T02:19:24.540756",
     "exception": false,
     "start_time": "2024-12-14T02:19:23.104321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mrmr import mrmr_classif\n",
    "import pandas as pd\n",
    "\n",
    "def feature_selector(num_features, X, y, classif_obj): # giữ lại kích thước chiều cuối cùng\n",
    "    flatted_X = X.reshape(-1, X.shape[-1])\n",
    "\n",
    "    flatted_X = pd.DataFrame(flatted_X)\n",
    "\n",
    "    flatted_y_new = y\n",
    "    if len(y.shape) == 2:\n",
    "        y_new = np.repeat(y[:, np.newaxis, :], X.shape[1], axis=1)\n",
    "        flatted_y_new = y_new.reshape(-1, y_new.shape[-1])\n",
    "        \n",
    "    flatted_y_new = flatted_y_new[:,classif_obj]\n",
    "\n",
    "    selected_features = mrmr_classif(X=flatted_X, y=flatted_y_new, K=num_features)\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# f = feature_selector(64, cwt_train_loaded, y_train, 0)\n",
    "# feature_selections = {\n",
    "#     'PCA': {\n",
    "#         'Valence': feature_selector(64, pca_train_loaded, y_train, 0), #1024x40x8064\n",
    "#         'Arousal': feature_selector(64, pca_train_loaded, y_train, 1) #1024x40x8064\n",
    "#     },\n",
    "#     'WT': {\n",
    "#         'Valence': feature_selector(64, cwt_train_loaded, y_train, 0), #1024x40x128\n",
    "#         'Arousal': feature_selector(64, cwt_train_loaded, y_train, 1) #1024x40x128\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Save file name\n",
    "# prefix_output_path = '/kaggle/working/'\n",
    "# filename = \"feature_selections.pkl\"\n",
    "\n",
    "# # Save data\n",
    "# with open(filename, 'wb') as file:\n",
    "#     pickle.dump(feature_selections, file)\n",
    "\n",
    "# print(f\"Data has been saved to file {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73794484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:24.553390Z",
     "iopub.status.busy": "2024-12-14T02:19:24.553020Z",
     "iopub.status.idle": "2024-12-14T02:19:24.569787Z",
     "shell.execute_reply": "2024-12-14T02:19:24.568889Z"
    },
    "papermill": {
     "duration": 0.024719,
     "end_time": "2024-12-14T02:19:24.571368",
     "exception": false,
     "start_time": "2024-12-14T02:19:24.546649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PCA': {'Valence': [62, 96, 53, 87, 15, 66, 83, 11, 113, 46, 125, 39, 38, 116, 6, 68, 56, 57, 36, 123, 126, 120, 69, 10, 51, 9, 119, 35, 99, 94, 43, 40, 42, 52, 90, 59, 16, 105, 118, 64, 27, 49, 63, 124, 101, 17, 98, 86, 5, 111, 81, 80, 33, 109, 45, 8, 61, 54, 1, 50, 112, 37, 29, 121], 'Arousal': [98, 59, 94, 9, 72, 97, 47, 56, 21, 17, 71, 83, 50, 5, 2, 39, 69, 49, 107, 37, 48, 65, 114, 27, 38, 20, 42, 11, 8, 112, 80, 81, 84, 14, 0, 91, 120, 1, 40, 118, 123, 28, 12, 86, 46, 104, 4, 74, 108, 3, 125, 88, 122, 55, 106, 29, 115, 30, 113, 66, 87, 119, 23, 41]}, 'WT': {'Valence': [52, 51, 53, 54, 50, 55, 56, 49, 57, 58, 48, 59, 1, 60, 61, 62, 63, 47, 64, 5, 65, 6, 66, 4, 67, 7, 68, 46, 0, 69, 8, 3, 70, 9, 10, 71, 30, 2, 31, 29, 11, 32, 17, 72, 16, 12, 18, 45, 15, 28, 13, 73, 14, 33, 19, 74, 20, 27, 34, 75, 21, 26, 35, 44], 'Arousal': [64, 63, 65, 66, 62, 67, 61, 68, 60, 69, 70, 59, 71, 58, 72, 73, 33, 57, 32, 34, 74, 31, 75, 56, 35, 76, 30, 36, 77, 55, 78, 37, 29, 79, 54, 80, 38, 81, 28, 53, 82, 39, 83, 27, 84, 52, 40, 85, 26, 86, 51, 41, 87, 25, 88, 42, 24, 50, 89, 23, 90, 43, 49, 22]}}\n"
     ]
    }
   ],
   "source": [
    "# Save file name\n",
    "prefix_path = 'C:\\\\Users\\\\ferri\\\\Downloads\\\\PoliTO\\\\Tesi\\\\DSs\\\\Stress\\\\deap-dataset\\\\'\n",
    "filename = \"feature_selections.pkl\"\n",
    "\n",
    "# Read data again\n",
    "with open(prefix_path + filename, 'rb') as file:\n",
    "    feature_selection_loaded = pickle.load(file)\n",
    "\n",
    "print(feature_selection_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33322c24",
   "metadata": {
    "papermill": {
     "duration": 0.005405,
     "end_time": "2024-12-14T02:19:24.582361",
     "exception": false,
     "start_time": "2024-12-14T02:19:24.576956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96b2179d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:24.594349Z",
     "iopub.status.busy": "2024-12-14T02:19:24.594092Z",
     "iopub.status.idle": "2024-12-14T02:19:24.802569Z",
     "shell.execute_reply": "2024-12-14T02:19:24.801686Z"
    },
    "papermill": {
     "duration": 0.216712,
     "end_time": "2024-12-14T02:19:24.804618",
     "exception": false,
     "start_time": "2024-12-14T02:19:24.587906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggregated_data = [\n",
    "    ['WT', 'YES', 'VALENCE', {'TRAIN': (cwt_train_loaded[:, :, feature_selection_loaded['WT']['Valence']], y_train), \n",
    "                              'TEST': (cwt_test_loaded[:, :, feature_selection_loaded['WT']['Valence']], y_test)}],\n",
    "    ['WT', 'YES', 'AROUSAL', {'TRAIN': (cwt_train_loaded[:, :, feature_selection_loaded['WT']['Arousal']], y_train),\n",
    "                              'TEST': (cwt_test_loaded[:, :, feature_selection_loaded['WT']['Arousal']], y_test)}],\n",
    "\n",
    "    ['WT', 'NO', '*', {'TRAIN': (cwt_train_loaded, y_train), \n",
    "                       'TEST': (cwt_test_loaded, y_test)}],\n",
    "\n",
    "    ['PE', 'NO', '*', {'TRAIN': (pe_train_loaded, y_train), \n",
    "                       'TEST': (pe_test_loaded, y_test)}],\n",
    "\n",
    "    ['NONE', 'YES', 'VALENCE', {'TRAIN': (pca_train_loaded[:, :, feature_selection_loaded['PCA']['Valence']], y_train), \n",
    "                                'TEST': (pca_test_loaded[:, :, feature_selection_loaded['PCA']['Valence']], y_test)}],\n",
    "    ['NONE', 'YES', 'AROUSAL', {'TRAIN': (pca_train_loaded[:, :, feature_selection_loaded['PCA']['Arousal']], y_train), \n",
    "                                'TEST': (pca_test_loaded[:, :, feature_selection_loaded['PCA']['Arousal']], y_test)}],\n",
    "\n",
    "    ['NONE', 'NO', '*', {'TRAIN': (pca_train_loaded, y_train), \n",
    "                         'TEST': (pca_test_loaded, y_test)}],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e60c4",
   "metadata": {
    "papermill": {
     "duration": 0.005819,
     "end_time": "2024-12-14T02:19:24.817106",
     "exception": false,
     "start_time": "2024-12-14T02:19:24.811287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f6c3902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:24.829502Z",
     "iopub.status.busy": "2024-12-14T02:19:24.829199Z",
     "iopub.status.idle": "2024-12-14T02:19:24.832997Z",
     "shell.execute_reply": "2024-12-14T02:19:24.832173Z"
    },
    "papermill": {
     "duration": 0.011764,
     "end_time": "2024-12-14T02:19:24.834625",
     "exception": false,
     "start_time": "2024-12-14T02:19:24.822861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#raw: 1024x40x8064 -> \n",
    "# feature extraction: 1024x40x128 -> \n",
    "# feature selection: 1024x5120 -> 1024xK\n",
    "\n",
    "# select each extraction method, selection for each model -> get the best result of each model to compare with each other\n",
    "# SVM, random forest: need to flatten input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "054779f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:24.846513Z",
     "iopub.status.busy": "2024-12-14T02:19:24.846253Z",
     "iopub.status.idle": "2024-12-14T02:19:27.922385Z",
     "shell.execute_reply": "2024-12-14T02:19:27.921487Z"
    },
    "papermill": {
     "duration": 3.084379,
     "end_time": "2024-12-14T02:19:27.924447",
     "exception": false,
     "start_time": "2024-12-14T02:19:24.840068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def calculate_metric(metric, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the value of a metric based on the input metric name.\n",
    "\n",
    "    Args:\n",
    "        metric (str): Metric name, one of the ['Acc', 'Sens', 'Spec', 'Prec', 'F-measure'].\n",
    "        y_true (array-like): Actual label.\n",
    "        y_pred (array-like): Predictive label.\n",
    "\n",
    "    Returns:\n",
    "        float: Value of selected metric.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the metric is not in the supported list.\n",
    "    \"\"\"\n",
    "    metric = metric.lower()  # Convert metrics to lowercase for easier handling\n",
    "\n",
    "    if metric == 'acc':\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif metric == 'sens':  # Sensitivity (Recall)\n",
    "        return recall_score(y_true, y_pred, average='binary')\n",
    "    elif metric == 'spec':  # Specificity\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        return tn / (tn + fp) if (tn + fp) != 0 else 0.0\n",
    "    elif metric == 'prec':  # Precision\n",
    "        return precision_score(y_true, y_pred, average='binary')\n",
    "    elif metric == 'f-measure':  # F1-Score\n",
    "        return f1_score(y_true, y_pred, average='binary')\n",
    "    else:\n",
    "        raise ValueError(f\"Metric '{metric}' not supported. Please select one of the ['Acc', 'Sens', 'Spec', 'Prec', 'F-measure'].\")\n",
    "    \n",
    "\n",
    "def evaluate(model, name_model, classifi_obj, X, y):\n",
    "\n",
    "    if name_model in ['svm', 'rf']:\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "    else:\n",
    "        X = torch.as_tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "        if name_model == 'rnn':\n",
    "            X = X.permute(0, 2, 1)\n",
    "        y_pred = model(X)\n",
    "        y_pred = (F.sigmoid(y_pred) >= 0.5).float().cpu().numpy()\n",
    "\n",
    "    result = {\n",
    "        metric: calculate_metric(\n",
    "            metric, \n",
    "            y[:, classifi_obj],\n",
    "            y_pred\n",
    "        )\n",
    "        for metric in ['Acc', 'Sens', 'Spec', 'Prec', 'F-measure']\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "169eec22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:27.937573Z",
     "iopub.status.busy": "2024-12-14T02:19:27.937191Z",
     "iopub.status.idle": "2024-12-14T02:19:27.955607Z",
     "shell.execute_reply": "2024-12-14T02:19:27.954818Z"
    },
    "papermill": {
     "duration": 0.026855,
     "end_time": "2024-12-14T02:19:27.957249",
     "exception": false,
     "start_time": "2024-12-14T02:19:27.930394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=1e-3, save_path='checkpoint.pth'):\n",
    "        \"\"\"\n",
    "        :param patience: number of waits before stopping (number of epochs where validation loss does not improve)\n",
    "        :param delta: minimum change in validation loss to be considered an improvement\n",
    "        :param save_path: path to save model when validation loss improves\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.save_path = save_path\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            # self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            # self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Save model when validation loss improves\"\"\"\n",
    "        torch.save(model.state_dict(), self.save_path)\n",
    "        print(f\"Validation loss decrease: {val_loss:.6f}, model saved.\")\n",
    "\n",
    "def setup_config_train_deep_model(train_data_X, train_data_y, test_data_X, test_data_y, name_model):\n",
    "    batch_size = 64\n",
    "    device=DEVICE\n",
    "    \n",
    "    # Create TensorDataset for train and test\n",
    "    train_dataset = TensorDataset(torch.as_tensor(train_data_X, device=device), torch.as_tensor(train_data_y, device=device))\n",
    "    test_dataset = TensorDataset(torch.as_tensor(test_data_X, device=device), torch.as_tensor(test_data_y, device=device))\n",
    "    \n",
    "    # Create DataLoader for train and test with batch_size=64 (or custom)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = 20\n",
    "    optimizer = Adam\n",
    "    lr = 1e-4\n",
    "    prefix_output_path = '/kaggle/working/'\n",
    "\n",
    "    config = {\n",
    "        'train_dataloader': train_dataloader,\n",
    "        'test_dataloader': test_dataloader,\n",
    "        'num_epochs': num_epochs,\n",
    "        'lr': lr,\n",
    "        'optimizer': optimizer,\n",
    "        'prefix_output_path': prefix_output_path,\n",
    "        'name_model': name_model\n",
    "    }\n",
    "\n",
    "    return config\n",
    "    \n",
    "def train_deep_model(model, classifi_obj, **kwargs):\n",
    "    name_model = kwargs.get('name_model')\n",
    "    lr = kwargs.get('lr')\n",
    "    optimizer = kwargs.get('optimizer')(model.parameters(), lr=lr)\n",
    "    num_epochs = kwargs.get('num_epochs')\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.9)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    log = \"\"\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_dataloader = kwargs.get('train_dataloader')\n",
    "        train_epoch_loss = []\n",
    "        \n",
    "        for i, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X_batch = X_batch.to(torch.float32)\n",
    "            y_batch = y_batch.to(torch.float32)\n",
    "            \n",
    "            if name_model.lower() == 'rnn':\n",
    "                X_batch = X_batch.permute(0,2,1)\n",
    "                \n",
    "            y_pred = model(X_batch)\n",
    "            \n",
    "            loss = criterion(y_pred, y_batch[:, classifi_obj])\n",
    "            train_epoch_loss.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_epoch_loss = np.mean(train_epoch_loss)\n",
    "        train_loss.append(train_epoch_loss)\n",
    "\n",
    "        # Calculate validation loss\n",
    "        test_dataloader = kwargs.get('test_dataloader')\n",
    "        model.eval()  # Switch to eval mode\n",
    "        val_epoch_loss = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_dataloader:\n",
    "                \n",
    "                X_batch = X_batch.to(torch.float32)\n",
    "                y_batch = y_batch.to(torch.float32)\n",
    "\n",
    "                if name_model.lower() == 'rnn':\n",
    "                    X_batch = X_batch.permute(0,2,1)\n",
    "                \n",
    "                outputs = model(X_batch)\n",
    "                \n",
    "                loss = criterion(outputs, y_batch[:, classifi_obj])\n",
    "                val_epoch_loss.append(loss.item())\n",
    "\n",
    "        val_epoch_loss = np.mean(val_epoch_loss)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "\n",
    "        prefix_output_path = kwargs.get('prefix_output_path')\n",
    "                \n",
    "        log += '\\n' + f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}, lr: {scheduler.get_last_lr()}\"\n",
    "        \n",
    "        early_stopping = EarlyStopping(val_loss, model, save_path=f'{prefix_output_path}{name_model}.pth')\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            # early_stopping.save_checkpoint(val_loss, model)\n",
    "            break\n",
    "            \n",
    "    # Save data\n",
    "    # with open(f'{prefix_output_path}log_{name_model}.txt', 'w') as file:\n",
    "    #     file.write(log)\n",
    "\n",
    "    return train_loss, val_loss, log\n",
    "    # pass\n",
    "\n",
    "def setup_config_train_ml_model(X_train, y_train, X_test, y_test, name_model):\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'name_model': name_model.lower()\n",
    "    }\n",
    "\n",
    "def train_ml_model(model, classifi_obj, **kwargs):\n",
    "    name_model = kwargs.get('name_model').lower()\n",
    "    X_train = kwargs.get('X_train')\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    y = kwargs.get('y_train')[:, classifi_obj]\n",
    "    model.fit(X_train, y)\n",
    "\n",
    "def setup_config_train(X_train, y_train, X_test, y_test, name_model):\n",
    "    name_model = name_model.lower()\n",
    "    if name_model in ['svm', 'rf']:\n",
    "        \n",
    "        return setup_config_train_ml_model(X_train, y_train, X_test, y_test, name_model)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return setup_config_train_deep_model(X_train, y_train, X_test, y_test, name_model)\n",
    "\n",
    "def train_model(model, classifi_obj, **kwargs):\n",
    "    if kwargs.get('name_model').lower() in ['svm', 'rf']:\n",
    "\n",
    "        return train_ml_model(model, classifi_obj, **kwargs)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return train_deep_model(model, classifi_obj, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e75362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:27.969415Z",
     "iopub.status.busy": "2024-12-14T02:19:27.969197Z",
     "iopub.status.idle": "2024-12-14T02:19:27.981569Z",
     "shell.execute_reply": "2024-12-14T02:19:27.980872Z"
    },
    "papermill": {
     "duration": 0.020523,
     "end_time": "2024-12-14T02:19:27.983154",
     "exception": false,
     "start_time": "2024-12-14T02:19:27.962631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, rnn_type=\"LSTM\", device=DEVICE):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Choose RNN type: LSTM, GRU, or vanilla RNN\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, device=device)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, device=device)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, device=device)\n",
    "\n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_size, output_size, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state (if using LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # Hidden state\n",
    "\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # Cell state\n",
    "            out, _ = self.rnn(x, (h0, c0))\n",
    "        else:\n",
    "            out, _ = self.rnn(x, h0)\n",
    "\n",
    "        # Take the last output of the sequence\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        out = out.squeeze()\n",
    "        \n",
    "        return out\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, outs_channels, output_size, length_signal, device=DEVICE):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = [in_channels] + outs_channels\n",
    "        self.model = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, device=device),\n",
    "                nn.BatchNorm1d(out_channels, device=device),\n",
    "                nn.ReLU()\n",
    "            ) for in_channels, out_channels in zip(channels[:-1], channels[1:])]\n",
    "        )\n",
    "            \n",
    "        self.fc = nn.Linear(outs_channels[-1] * length_signal, output_size, device=device) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = x.squeeze()\n",
    "\n",
    "        return x\n",
    "    \n",
    "def get_model(**kwargs):\n",
    "    model = None\n",
    "    name_model = kwargs.get('name_model').lower()\n",
    "    \n",
    "    device = kwargs.get('device', DEVICE)\n",
    "    \n",
    "    if name_model == 'svm':\n",
    "        model = SVC(kernel='rbf')\n",
    "        \n",
    "    elif name_model == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_leaf=5, random_state=42)\n",
    "        \n",
    "    elif name_model == 'rnn':\n",
    "        # with torch.no_grad():\n",
    "        #     dummy = torch.Tensor(1024, 40, 128).permute(0, 2, 1)# -> 1024, 128, 40\n",
    "        input_features = kwargs.get('input_features', 40)\n",
    "        hidden_size = kwargs.get('hidden_size', 128)\n",
    "        num_layers = kwargs.get('num_layers', 4)\n",
    "        output_size = kwargs.get('output_size', 1) # 1 output unit for binary classification\n",
    "        length_signal = kwargs.get('length_signal', 128)\n",
    "        \n",
    "        model = RNNModel(\n",
    "            input_size=input_features, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            output_size=output_size, \n",
    "            rnn_type=\"LSTM\"\n",
    "        )\n",
    "        \n",
    "    elif name_model == 'cnn':\n",
    "        in_channels = kwargs.get('in_channels', 40)\n",
    "        outs_channels = kwargs.get('outs_channels', [64,256,128])\n",
    "        output_size = kwargs.get('output_size', 1)\n",
    "        length_signal = kwargs.get('length_signal', 128)\n",
    "        \n",
    "        model = CNNModel(\n",
    "            in_channels=in_channels,\n",
    "            outs_channels=outs_channels,\n",
    "            output_size=output_size,\n",
    "            length_signal=length_signal,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c325a052",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:27.995071Z",
     "iopub.status.busy": "2024-12-14T02:19:27.994833Z",
     "iopub.status.idle": "2024-12-14T02:19:27.998405Z",
     "shell.execute_reply": "2024-12-14T02:19:27.997609Z"
    },
    "papermill": {
     "duration": 0.011267,
     "end_time": "2024-12-14T02:19:27.999927",
     "exception": false,
     "start_time": "2024-12-14T02:19:27.988660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_data_train, y_data_train = aggregated_data[0][-1]['TRAIN']\n",
    "# X_data_test, y_data_test = aggregated_data[0][-1]['TEST']\n",
    "\n",
    "# name_model = 'svm'\n",
    "# classifi_obj = 0\n",
    "\n",
    "# config = {'name_model': name_model, 'length_signal': 64}\n",
    "# model = get_model(**config)\n",
    "\n",
    "# config_train = setup_config_train(X_data_train, y_data_train, X_data_test, y_data_test, name_model)\n",
    "# # print(config_train)\n",
    "# info = train_model(model, classifi_obj, **config_train)\n",
    "# print(info)\n",
    "\n",
    "# print(evaluate(model, name_model, classifi_obj, X_data_test, y_data_test))\n",
    "# # out = model(X)\n",
    "# # print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cbba155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T02:19:28.011763Z",
     "iopub.status.busy": "2024-12-14T02:19:28.011485Z",
     "iopub.status.idle": "2024-12-14T02:21:05.570402Z",
     "shell.execute_reply": "2024-12-14T02:21:05.569408Z"
    },
    "papermill": {
     "duration": 97.566902,
     "end_time": "2024-12-14T02:21:05.572239",
     "exception": false,
     "start_time": "2024-12-14T02:19:28.005337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 1: ('rf', 'WT', 'YES', 'AROUSAL', 78.125)\n",
      "experiment 2: ('rf', 'WT', 'YES', 'VALENCE', 76.953125)\n",
      "experiment 3: ('rf', 'WT', 'NO', 'AROUSAL', 78.125)\n",
      "experiment 4: ('rf', 'WT', 'NO', 'VALENCE', 76.5625)\n",
      "experiment 5: ('rf', 'PE', 'NO', 'AROUSAL', 78.90625)\n",
      "experiment 6: ('rf', 'PE', 'NO', 'VALENCE', 77.34375)\n",
      "experiment 7: ('rf', 'NONE', 'YES', 'AROUSAL', 77.34375)\n",
      "experiment 8: ('rf', 'NONE', 'YES', 'VALENCE', 68.75)\n",
      "experiment 9: ('rf', 'NONE', 'NO', 'AROUSAL', 78.125)\n",
      "experiment 10: ('rf', 'NONE', 'NO', 'VALENCE', 71.875)\n",
      "experiment 11: ('svm', 'WT', 'YES', 'AROUSAL', 78.515625)\n",
      "experiment 12: ('svm', 'WT', 'YES', 'VALENCE', 77.34375)\n",
      "experiment 13: ('svm', 'WT', 'NO', 'AROUSAL', 78.515625)\n",
      "experiment 14: ('svm', 'WT', 'NO', 'VALENCE', 77.34375)\n",
      "experiment 15: ('svm', 'PE', 'NO', 'AROUSAL', 79.296875)\n",
      "experiment 16: ('svm', 'PE', 'NO', 'VALENCE', 77.34375)\n",
      "experiment 17: ('svm', 'NONE', 'YES', 'AROUSAL', 79.296875)\n",
      "experiment 18: ('svm', 'NONE', 'YES', 'VALENCE', 77.34375)\n",
      "experiment 19: ('svm', 'NONE', 'NO', 'AROUSAL', 79.296875)\n",
      "experiment 20: ('svm', 'NONE', 'NO', 'VALENCE', 77.34375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:30<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 21: ('rnn', 'WT', 'YES', 'AROUSAL', 79.296875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:30<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 22: ('rnn', 'WT', 'YES', 'VALENCE', 76.953125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:00<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 23: ('rnn', 'WT', 'NO', 'AROUSAL', 79.296875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:01<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 24: ('rnn', 'WT', 'NO', 'VALENCE', 77.34375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 25: ('rnn', 'PE', 'NO', 'AROUSAL', 79.296875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 26: ('rnn', 'PE', 'NO', 'VALENCE', 77.34375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 27: ('rnn', 'NONE', 'YES', 'AROUSAL', 79.296875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:33<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 28: ('rnn', 'NONE', 'YES', 'VALENCE', 77.34375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:16<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 29: ('rnn', 'NONE', 'NO', 'AROUSAL', 79.296875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:02<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 30: ('rnn', 'NONE', 'NO', 'VALENCE', 77.34375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 31: ('cnn', 'WT', 'YES', 'AROUSAL', 78.515625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 32: ('cnn', 'WT', 'YES', 'VALENCE', 77.34375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 33: ('cnn', 'WT', 'NO', 'AROUSAL', 78.125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 34: ('cnn', 'WT', 'NO', 'VALENCE', 77.34375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 35: ('cnn', 'PE', 'NO', 'AROUSAL', 78.90625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 36: ('cnn', 'PE', 'NO', 'VALENCE', 74.609375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 37: ('cnn', 'NONE', 'YES', 'AROUSAL', 78.125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 38: ('cnn', 'NONE', 'YES', 'VALENCE', 75.390625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 39: ('cnn', 'NONE', 'NO', 'AROUSAL', 78.90625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 40: ('cnn', 'NONE', 'NO', 'VALENCE', 75.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "evaluates = []\n",
    "i = 0\n",
    "for name_model in ['rf', 'svm', 'rnn', 'cnn']:\n",
    "    for experiment in aggregated_data:\n",
    "        data = experiment[-1]\n",
    "        classifi_objs = [0] if experiment[2] == 'VALENCE' else [1] if experiment[2] == 'AROUSAL' else [0,1]\n",
    "        (X_train_data, y_train_data), (X_test_data, y_test_data) = data['TRAIN'], data['TEST']\n",
    "        # print(X_test_data.shape, X_train_data.shape)\n",
    "        for classifi_obj in classifi_objs:\n",
    "            config = {\n",
    "                'length_signal': X_train_data.shape[-1],\n",
    "                'name_model': name_model\n",
    "            }\n",
    "            \n",
    "            model = get_model(**config)\n",
    "\n",
    "            # print(name_model)\n",
    "            config_train = setup_config_train(X_train_data, y_train_data, X_test_data, y_test_data, name_model)\n",
    "            train_model(model, classifi_obj, **config_train)\n",
    "\n",
    "            motion = 'VALENCE' if classifi_obj else 'AROUSAL'\n",
    "\n",
    "            models.append((*experiment[:2], motion, model))\n",
    "\n",
    "            info = (\n",
    "                name_model,\n",
    "                *experiment[:2], \n",
    "                motion,\n",
    "                evaluate(model, name_model, classifi_obj, X_test_data, y_test_data)['Acc']*100\n",
    "            )\n",
    "            i += 1\n",
    "            print(f'experiment {i}: {info}')\n",
    "            evaluates.append(info)\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6241375,
     "sourceId": 10115927,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6257838,
     "sourceId": 10142875,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6260624,
     "sourceId": 10143051,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6268504,
     "sourceId": 10153382,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6281493,
     "sourceId": 10170850,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 154.627177,
   "end_time": "2024-12-14T02:21:07.015809",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-14T02:18:32.388632",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
